{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:2.18.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "print('TensorFlow version:{}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images shape: (60000, 28, 28)\n",
      "train_images type: <class 'numpy.ndarray'>\n",
      "train_labels shape: (60000,)\n",
      "train_labels type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print('train_images shape:', train_images.shape)\n",
    "print('train_images type:', type(train_images))\n",
    "print('train_labels shape:', train_labels.shape)\n",
    "print('train_labels type:', type(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images shape: (60000, 28, 28, 1)\n",
      "test_images shape: (10000, 28, 28, 1)\n",
      "train_images dtype: float64\n",
      "test_images dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# scale the values to 0.0 to 1.0\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# reshape for feeding into the model\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
    "\n",
    "print('train_images shape:', train_images.shape)\n",
    "print('test_images shape:', test_images.shape)\n",
    "\n",
    "print('train_images dtype:', train_images.dtype)\n",
    "print('test_images dtype:', test_images.dtype)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dgmr2/notebook/tensorflow/.venv/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1352</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">13,530</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Conv1 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │            \u001b[38;5;34m80\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1352\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Softmax (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m13,530\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,610</span> (53.16 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,610\u001b[0m (53.16 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,610</span> (53.16 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,610\u001b[0m (53.16 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7534 - loss: 0.7286\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 972us/step - accuracy: 0.8601 - loss: 0.3980\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 924us/step - accuracy: 0.8746 - loss: 0.3587\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 918us/step - accuracy: 0.8805 - loss: 0.3379\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 924us/step - accuracy: 0.8854 - loss: 0.3198\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - accuracy: 0.8797 - loss: 0.3469\n",
      "Test accuracy:0.8762000203132629\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(input_shape=(28, 28, 1), filters=8, kernel_size=3, strides=2, activation='relu', name='Conv1'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax, name='Softmax')\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "testing = False\n",
    "epochs = 5\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=epochs)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy:{test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Sequential in module keras.src.models.sequential object:\n",
      "\n",
      "class Sequential(keras.src.models.model.Model)\n",
      " |  Sequential(*args, **kwargs)\n",
      " |  \n",
      " |  `Sequential` groups a linear stack of layers into a `Model`.\n",
      " |  \n",
      " |  Examples:\n",
      " |  \n",
      " |  ```python\n",
      " |  model = keras.Sequential()\n",
      " |  model.add(keras.Input(shape=(16,)))\n",
      " |  model.add(keras.layers.Dense(8))\n",
      " |  \n",
      " |  # Note that you can also omit the initial `Input`.\n",
      " |  # In that case the model doesn't have any weights until the first call\n",
      " |  # to a training/evaluation method (since it isn't yet built):\n",
      " |  model = keras.Sequential()\n",
      " |  model.add(keras.layers.Dense(8))\n",
      " |  model.add(keras.layers.Dense(4))\n",
      " |  # model.weights not created yet\n",
      " |  \n",
      " |  # Whereas if you specify an `Input`, the model gets built\n",
      " |  # continuously as you are adding layers:\n",
      " |  model = keras.Sequential()\n",
      " |  model.add(keras.Input(shape=(16,)))\n",
      " |  model.add(keras.layers.Dense(8))\n",
      " |  len(model.weights)  # Returns \"2\"\n",
      " |  \n",
      " |  # When using the delayed-build pattern (no input shape specified), you can\n",
      " |  # choose to manually build your model by calling\n",
      " |  # `build(batch_input_shape)`:\n",
      " |  model = keras.Sequential()\n",
      " |  model.add(keras.layers.Dense(8))\n",
      " |  model.add(keras.layers.Dense(4))\n",
      " |  model.build((None, 16))\n",
      " |  len(model.weights)  # Returns \"4\"\n",
      " |  \n",
      " |  # Note that when using the delayed-build pattern (no input shape specified),\n",
      " |  # the model gets built the first time you call `fit`, `eval`, or `predict`,\n",
      " |  # or the first time you call the model on some input data.\n",
      " |  model = keras.Sequential()\n",
      " |  model.add(keras.layers.Dense(8))\n",
      " |  model.add(keras.layers.Dense(1))\n",
      " |  model.compile(optimizer='sgd', loss='mse')\n",
      " |  # This builds the model for the first time:\n",
      " |  model.fit(x, y, batch_size=32, epochs=10)\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Sequential\n",
      " |      keras.src.models.model.Model\n",
      " |      keras.src.backend.tensorflow.trainer.TensorFlowTrainer\n",
      " |      keras.src.trainers.trainer.Trainer\n",
      " |      keras.src.layers.layer.Layer\n",
      " |      keras.src.backend.tensorflow.layer.TFLayer\n",
      " |      keras.src.backend.tensorflow.trackable.KerasAutoTrackable\n",
      " |      tensorflow.python.trackable.autotrackable.AutoTrackable\n",
      " |      tensorflow.python.trackable.base.Trackable\n",
      " |      keras.src.ops.operation.Operation\n",
      " |      keras.src.saving.keras_saveable.KerasSaveable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, layers=None, trainable=True, name=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  add(self, layer, rebuild=True)\n",
      " |      Adds a layer instance on top of the layer stack.\n",
      " |      \n",
      " |      Args:\n",
      " |          layer: layer instance.\n",
      " |  \n",
      " |  build(self, input_shape=None)\n",
      " |  \n",
      " |  call(self, inputs, training=None, mask=None)\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |  \n",
      " |  compute_output_spec(self, inputs, training=None, mask=None)\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the object.\n",
      " |      \n",
      " |      An object config is a Python dictionary (serializable)\n",
      " |      containing the information needed to re-instantiate it.\n",
      " |  \n",
      " |  pop(self, rebuild=True)\n",
      " |      Removes the last layer in the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_config(config, custom_objects=None) from builtins.type\n",
      " |      Creates an operation from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`, capable of instantiating the\n",
      " |      same operation from the config dictionary.\n",
      " |      \n",
      " |      Note: If you override this method, you might receive a serialized dtype\n",
      " |      config, which is a `dict`. You can deserialize it as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      if \"dtype\" in config and isinstance(config[\"dtype\"], dict):\n",
      " |          policy = dtype_policies.deserialize(config[\"dtype\"])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the output of `get_config`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An operation instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  input_dtype\n",
      " |      The dtype layer inputs should be converted to.\n",
      " |  \n",
      " |  input_shape\n",
      " |  \n",
      " |  inputs\n",
      " |  \n",
      " |  output_shape\n",
      " |  \n",
      " |  outputs\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  layers\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.models.model.Model:\n",
      " |  \n",
      " |  build_from_config(self, config)\n",
      " |      Builds the layer's states with the supplied config dict.\n",
      " |      \n",
      " |      By default, this method calls the `build(config[\"input_shape\"])` method,\n",
      " |      which creates weights based on the layer's input shape in the supplied\n",
      " |      config. If your config contains other information needed to load the\n",
      " |      layer's state, you should override this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: Dict containing the input shape associated with this layer.\n",
      " |  \n",
      " |  export(self, filepath, format='tf_saved_model', verbose=True, input_signature=None, **kwargs)\n",
      " |      Export the model as an artifact for inference.\n",
      " |      \n",
      " |      Args:\n",
      " |          filepath: `str` or `pathlib.Path` object. The path to save the\n",
      " |              artifact.\n",
      " |          format: `str`. The export format. Supported values:\n",
      " |              `\"tf_saved_model\"` and `\"onnx\"`.  Defaults to\n",
      " |              `\"tf_saved_model\"`.\n",
      " |          verbose: `bool`. Whether to print a message during export. Defaults\n",
      " |              to `True`.\n",
      " |          input_signature: Optional. Specifies the shape and dtype of the\n",
      " |              model inputs. Can be a structure of `keras.InputSpec`,\n",
      " |              `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If\n",
      " |              not provided, it will be automatically computed. Defaults to\n",
      " |              `None`.\n",
      " |          **kwargs: Additional keyword arguments:\n",
      " |              - Specific to the JAX backend and `format=\"tf_saved_model\"`:\n",
      " |                  - `is_static`: Optional `bool`. Indicates whether `fn` is\n",
      " |                      static. Set to `False` if `fn` involves state updates\n",
      " |                      (e.g., RNG seeds and counters).\n",
      " |                  - `jax2tf_kwargs`: Optional `dict`. Arguments for\n",
      " |                      `jax2tf.convert`. See the documentation for\n",
      " |                      [`jax2tf.convert`](\n",
      " |                          https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).\n",
      " |                      If `native_serialization` and `polymorphic_shapes` are\n",
      " |                      not provided, they will be automatically computed.\n",
      " |      \n",
      " |      **Note:** This feature is currently supported only with TensorFlow, JAX\n",
      " |      and Torch backends.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      Here's how to export a TensorFlow SavedModel for inference.\n",
      " |      \n",
      " |      ```python\n",
      " |      # Export the model as a TensorFlow SavedModel artifact\n",
      " |      model.export(\"path/to/location\", format=\"tf_saved_model\")\n",
      " |      \n",
      " |      # Load the artifact in a different process/environment\n",
      " |      reloaded_artifact = tf.saved_model.load(\"path/to/location\")\n",
      " |      predictions = reloaded_artifact.serve(input_data)\n",
      " |      ```\n",
      " |      \n",
      " |      Here's how to export an ONNX for inference.\n",
      " |      \n",
      " |      ```python\n",
      " |      # Export the model as a ONNX artifact\n",
      " |      model.export(\"path/to/location\", format=\"onnx\")\n",
      " |      \n",
      " |      # Load the artifact in a different process/environment\n",
      " |      ort_session = onnxruntime.InferenceSession(\"path/to/location\")\n",
      " |      ort_inputs = {\n",
      " |          k.name: v for k, v in zip(ort_session.get_inputs(), input_data)\n",
      " |      }\n",
      " |      predictions = ort_session.run(None, ort_inputs)\n",
      " |      ```\n",
      " |  \n",
      " |  get_layer(self, name=None, index=None)\n",
      " |      Retrieves a layer based on either its name (unique) or index.\n",
      " |      \n",
      " |      If `name` and `index` are both provided, `index` will take precedence.\n",
      " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
      " |      \n",
      " |      Args:\n",
      " |          name: String, name of layer.\n",
      " |          index: Integer, index of layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  get_state_tree(self, value_format='backend_tensor')\n",
      " |      Retrieves tree-like structure of model variables.\n",
      " |      \n",
      " |      This method allows retrieval of different model variables (trainable,\n",
      " |      non-trainable, optimizer, and metrics). The variables are returned in a\n",
      " |      nested dictionary format, where the keys correspond to the variable\n",
      " |      names and the values are the nested representations of the variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict: A dictionary containing the nested representations of the\n",
      " |              requested variables. The keys are the variable names, and the\n",
      " |              values are the corresponding nested dictionaries.\n",
      " |          value_format: One of `\"backend_tensor\"`, `\"numpy_array\"`.\n",
      " |              The kind of array to return as the leaves of the nested\n",
      " |                  state tree.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      model = keras.Sequential([\n",
      " |          keras.Input(shape=(1,), name=\"my_input\"),\n",
      " |          keras.layers.Dense(1, activation=\"sigmoid\", name=\"my_dense\"),\n",
      " |      ], name=\"my_sequential\")\n",
      " |      model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
      " |      model.fit(np.array([[1.0]]), np.array([[1.0]]))\n",
      " |      state_tree = model.get_state_tree()\n",
      " |      ```\n",
      " |      \n",
      " |      The `state_tree` dictionary returned looks like:\n",
      " |      \n",
      " |      ```\n",
      " |      {\n",
      " |          'metrics_variables': {\n",
      " |              'loss': {\n",
      " |                  'count': ...,\n",
      " |                  'total': ...,\n",
      " |              },\n",
      " |              'mean_absolute_error': {\n",
      " |                  'count': ...,\n",
      " |                  'total': ...,\n",
      " |              }\n",
      " |          },\n",
      " |          'trainable_variables': {\n",
      " |              'my_sequential': {\n",
      " |                  'my_dense': {\n",
      " |                      'bias': ...,\n",
      " |                      'kernel': ...,\n",
      " |                  }\n",
      " |              }\n",
      " |          },\n",
      " |          'non_trainable_variables': {},\n",
      " |          'optimizer_variables': {\n",
      " |              'adam': {\n",
      " |                      'iteration': ...,\n",
      " |                      'learning_rate': ...,\n",
      " |                      'my_sequential_my_dense_bias_momentum': ...,\n",
      " |                      'my_sequential_my_dense_bias_velocity': ...,\n",
      " |                      'my_sequential_my_dense_kernel_momentum': ...,\n",
      " |                      'my_sequential_my_dense_kernel_velocity': ...,\n",
      " |                  }\n",
      " |              }\n",
      " |          }\n",
      " |      }\n",
      " |      ```\n",
      " |  \n",
      " |  load_weights(self, filepath, skip_mismatch=False, **kwargs)\n",
      " |      Load weights from a file saved via `save_weights()`.\n",
      " |      \n",
      " |      Weights are loaded based on the network's\n",
      " |      topology. This means the architecture should be the same as when the\n",
      " |      weights were saved. Note that layers that don't have weights are not\n",
      " |      taken into account in the topological ordering, so adding or removing\n",
      " |      layers is fine as long as they don't have weights.\n",
      " |      \n",
      " |      **Partial weight loading**\n",
      " |      \n",
      " |      If you have modified your model, for instance by adding a new layer\n",
      " |      (with weights) or by changing the shape of the weights of a layer,\n",
      " |      you can choose to ignore errors and continue loading\n",
      " |      by setting `skip_mismatch=True`. In this case any layer with\n",
      " |      mismatching weights will be skipped. A warning will be displayed\n",
      " |      for each skipped layer.\n",
      " |      \n",
      " |      Args:\n",
      " |          filepath: String, path to the weights file to load.\n",
      " |              It can either be a `.weights.h5` file\n",
      " |              or a legacy `.h5` weights file.\n",
      " |          skip_mismatch: Boolean, whether to skip loading of layers where\n",
      " |              there is a mismatch in the number of weights, or a mismatch in\n",
      " |              the shape of the weights.\n",
      " |  \n",
      " |  quantize(self, mode, **kwargs)\n",
      " |      Quantize the weights of the model.\n",
      " |      \n",
      " |      Note that the model must be built first before calling this method.\n",
      " |      `quantize` will recursively call `quantize(mode)` in all layers and\n",
      " |      will be skipped if the layer doesn't implement the function.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode: The mode of the quantization. Only 'int8' is supported at this\n",
      " |              time.\n",
      " |  \n",
      " |  save(self, filepath, overwrite=True, zipped=None, **kwargs)\n",
      " |      Saves a model as a `.keras` file.\n",
      " |      \n",
      " |      Args:\n",
      " |          filepath: `str` or `pathlib.Path` object.\n",
      " |              The path where to save the model. Must end in `.keras`\n",
      " |              (unless saving the model as an unzipped directory\n",
      " |              via `zipped=False`).\n",
      " |          overwrite: Whether we should overwrite any existing model at\n",
      " |              the target location, or instead ask the user via\n",
      " |              an interactive prompt.\n",
      " |          zipped: Whether to save the model as a zipped `.keras`\n",
      " |              archive (default when saving locally), or as an\n",
      " |              unzipped directory (default when saving on the\n",
      " |              Hugging Face Hub).\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      model = keras.Sequential(\n",
      " |          [\n",
      " |              keras.layers.Dense(5, input_shape=(3,)),\n",
      " |              keras.layers.Softmax(),\n",
      " |          ],\n",
      " |      )\n",
      " |      model.save(\"model.keras\")\n",
      " |      loaded_model = keras.saving.load_model(\"model.keras\")\n",
      " |      x = keras.random.uniform((10, 3))\n",
      " |      assert np.allclose(model.predict(x), loaded_model.predict(x))\n",
      " |      ```\n",
      " |      \n",
      " |      Note that `model.save()` is an alias for `keras.saving.save_model()`.\n",
      " |      \n",
      " |      The saved `.keras` file contains:\n",
      " |      \n",
      " |      - The model's configuration (architecture)\n",
      " |      - The model's weights\n",
      " |      - The model's optimizer's state (if any)\n",
      " |      \n",
      " |      Thus models can be reinstantiated in the exact same state.\n",
      " |  \n",
      " |  save_weights(self, filepath, overwrite=True)\n",
      " |      Saves all layer weights to a `.weights.h5` file.\n",
      " |      \n",
      " |      Args:\n",
      " |          filepath: `str` or `pathlib.Path` object.\n",
      " |              Path where to save the model. Must end in `.weights.h5`.\n",
      " |          overwrite: Whether we should overwrite any existing model\n",
      " |              at the target location, or instead ask the user\n",
      " |              via an interactive prompt.\n",
      " |  \n",
      " |  set_state_tree(self, state_tree)\n",
      " |      Assigns values to variables of the model.\n",
      " |      \n",
      " |      This method takes a dictionary of nested variable values, which\n",
      " |      represents the state tree of the model, and assigns them to the\n",
      " |      corresponding variables of the model. The dictionary keys represent the\n",
      " |      variable names (e.g., `'trainable_variables'`, `'optimizer_variables'`),\n",
      " |      and the values are nested dictionaries containing the variable\n",
      " |      paths and their corresponding values.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_tree: A dictionary representing the state tree of the model.\n",
      " |              The keys are the variable names, and the values are nested\n",
      " |              dictionaries representing the variable paths and their values.\n",
      " |  \n",
      " |  summary(self, line_length=None, positions=None, print_fn=None, expand_nested=False, show_trainable=False, layer_range=None)\n",
      " |      Prints a string summary of the network.\n",
      " |      \n",
      " |      Args:\n",
      " |          line_length: Total length of printed lines\n",
      " |              (e.g. set this to adapt the display to different\n",
      " |              terminal window sizes).\n",
      " |          positions: Relative or absolute positions of log elements\n",
      " |              in each line. If not provided, becomes\n",
      " |              `[0.3, 0.6, 0.70, 1.]`. Defaults to `None`.\n",
      " |          print_fn: Print function to use. By default, prints to `stdout`.\n",
      " |              If `stdout` doesn't work in your environment, change to `print`.\n",
      " |              It will be called on each line of the summary.\n",
      " |              You can set it to a custom function\n",
      " |              in order to capture the string summary.\n",
      " |          expand_nested: Whether to expand the nested models.\n",
      " |              Defaults to `False`.\n",
      " |          show_trainable: Whether to show if a layer is trainable.\n",
      " |              Defaults to `False`.\n",
      " |          layer_range: a list or tuple of 2 strings,\n",
      " |              which is the starting layer name and ending layer name\n",
      " |              (both inclusive) indicating the range of layers to be printed\n",
      " |              in summary. It also accepts regex patterns instead of exact\n",
      " |              names. In this case, the start predicate will be\n",
      " |              the first element that matches `layer_range[0]`\n",
      " |              and the end predicate will be the last element\n",
      " |              that matches `layer_range[1]`.\n",
      " |              By default `None` considers all layers of the model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if `summary()` is called before the model is built.\n",
      " |  \n",
      " |  to_json(self, **kwargs)\n",
      " |      Returns a JSON string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a JSON save file, use\n",
      " |      `keras.models.model_from_json(json_string, custom_objects={...})`.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Additional keyword arguments to be passed to\n",
      " |              `json.dumps()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.backend.tensorflow.trainer.TensorFlowTrainer:\n",
      " |  \n",
      " |  compiled_loss(self, y, y_pred, sample_weight=None, regularization_losses=None)\n",
      " |  \n",
      " |  evaluate(self, x=None, y=None, batch_size=None, verbose='auto', sample_weight=None, steps=None, callbacks=None, return_dict=False, **kwargs)\n",
      " |      Returns the loss value & metrics values for the model in test mode.\n",
      " |      \n",
      " |      Computation is done in batches (see the `batch_size` arg.)\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data. It can be:\n",
      " |              - A NumPy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |              - A backend-native tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |              - A dict mapping input names to the corresponding array/tensors,\n",
      " |              if the model has named inputs.\n",
      " |              - A `keras.utils.PyDataset` returning `(inputs, targets)` or\n",
      " |              `(inputs, targets, sample_weights)`.\n",
      " |              - A `tf.data.Dataset` yielding `(inputs, targets)` or\n",
      " |              `(inputs, targets, sample_weights)`.\n",
      " |              - A `torch.utils.data.DataLoader` yielding `(inputs, targets)`\n",
      " |              or `(inputs, targets, sample_weights)`.\n",
      " |              - A Python generator function yielding `(inputs, targets)` or\n",
      " |              `(inputs, targets, sample_weights)`.\n",
      " |          y: Target data. Like the input data `x`, it can be either NumPy\n",
      " |              array(s) or backend-native tensor(s). If `x` is a\n",
      " |              `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
      " |              `torch.utils.data.DataLoader` or a Python generator function,\n",
      " |              `y` should not be specified since targets will be obtained from\n",
      " |              `x`.\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per batch of computation.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |              Do not specify the `batch_size` if your input data `x` is a\n",
      " |              `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
      " |              `torch.utils.data.DataLoader` or Python generator function\n",
      " |              since they generate batches.\n",
      " |          verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = single line.\n",
      " |              `\"auto\"` becomes 1 for most cases.\n",
      " |              Note that the progress bar is not\n",
      " |              particularly useful when logged to a file, so `verbose=2` is\n",
      " |              recommended when not running interactively\n",
      " |              (e.g. in a production environment). Defaults to `\"auto\"`.\n",
      " |          sample_weight: Optional NumPy array or tensor of weights for\n",
      " |              the training samples, used for weighting the loss function\n",
      " |              (during training only). You can either pass a flat (1D)\n",
      " |              NumPy array or tensor with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples), or in the case of\n",
      " |              temporal data, you can pass a 2D NumPy array or tensor with\n",
      " |              shape `(samples, sequence_length)` to apply a different weight\n",
      " |              to every timestep of every sample.\n",
      " |              This argument is not supported when `x` is a\n",
      " |              `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
      " |              `torch.utils.data.DataLoader` or Python generator function.\n",
      " |              Instead, provide `sample_weights` as the third element of `x`.\n",
      " |              Note that sample weighting does not apply to metrics specified\n",
      " |              via the `metrics` argument in `compile()`. To apply sample\n",
      " |              weighting to your metrics, you can specify them via the\n",
      " |              `weighted_metrics` in `compile()` instead.\n",
      " |          steps: Integer or `None`.\n",
      " |              Total number of steps (batches of samples) to draw before\n",
      " |              declaring the evaluation round finished. If `steps` is `None`,\n",
      " |              it will run until `x` is exhausted. In the case of an infinitely\n",
      " |              repeating dataset, it will run indefinitely.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during evaluation.\n",
      " |          return_dict: If `True`, loss and metric results are returned as a\n",
      " |              dict, with each key being the name of the metric.\n",
      " |              If `False`, they are returned as a list.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |  \n",
      " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose='auto', callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1)\n",
      " |      Trains the model for a fixed number of epochs (dataset iterations).\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data. It can be:\n",
      " |              - A NumPy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |              - A backend-native tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |              - A dict mapping input names to the corresponding array/tensors,\n",
      " |              if the model has named inputs.\n",
      " |              - A `keras.utils.PyDataset` returning `(inputs, targets)` or\n",
      " |              `(inputs, targets, sample_weights)`.\n",
      " |              - A `tf.data.Dataset` yielding `(inputs, targets)` or\n",
      " |              `(inputs, targets, sample_weights)`.\n",
      " |              - A `torch.utils.data.DataLoader` yielding `(inputs, targets)`\n",
      " |              or `(inputs, targets, sample_weights)`.\n",
      " |              - A Python generator function yielding `(inputs, targets)` or\n",
      " |              `(inputs, targets, sample_weights)`.\n",
      " |          y: Target data. Like the input data `x`, it can be either NumPy\n",
      " |              array(s) or backend-native tensor(s). If `x` is a\n",
      " |              `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
      " |              `torch.utils.data.DataLoader` or a Python generator function,\n",
      " |              `y` should not be specified since targets will be obtained from\n",
      " |              `x`.\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per gradient update.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |              Do not specify the `batch_size` if your input data `x` is a\n",
      " |              `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
      " |              `torch.utils.data.DataLoader` or Python generator function\n",
      " |              since they generate batches.\n",
      " |          epochs: Integer. Number of epochs to train the model.\n",
      " |              An epoch is an iteration over the entire `x` and `y`\n",
      " |              data provided\n",
      " |              (unless the `steps_per_epoch` flag is set to\n",
      " |              something other than None).\n",
      " |              Note that in conjunction with `initial_epoch`,\n",
      " |              `epochs` is to be understood as \"final epoch\".\n",
      " |              The model is not trained for a number of iterations\n",
      " |              given by `epochs`, but merely until the epoch\n",
      " |              of index `epochs` is reached.\n",
      " |          verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      " |              \"auto\" becomes 1 for most cases.\n",
      " |              Note that the progress bar is not\n",
      " |              particularly useful when logged to a file,\n",
      " |              so `verbose=2` is recommended when not running interactively\n",
      " |              (e.g., in a production environment). Defaults to `\"auto\"`.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during training.\n",
      " |              See `keras.callbacks`. Note\n",
      " |              `keras.callbacks.ProgbarLogger` and\n",
      " |              `keras.callbacks.History` callbacks are created\n",
      " |              automatically and need not be passed to `model.fit()`.\n",
      " |              `keras.callbacks.ProgbarLogger` is created\n",
      " |              or not based on the `verbose` argument in `model.fit()`.\n",
      " |          validation_split: Float between 0 and 1.\n",
      " |              Fraction of the training data to be used as validation data.\n",
      " |              The model will set apart this fraction of the training data,\n",
      " |              will not train on it, and will evaluate the loss and any model\n",
      " |              metrics on this data at the end of each epoch. The validation\n",
      " |              data is selected from the last samples in the `x` and `y` data\n",
      " |              provided, before shuffling.\n",
      " |              This argument is only supported when `x` and `y` are made of\n",
      " |              NumPy arrays or tensors.\n",
      " |              If both `validation_data` and `validation_split` are provided,\n",
      " |              `validation_data` will override `validation_split`.\n",
      " |          validation_data: Data on which to evaluate\n",
      " |              the loss and any model metrics at the end of each epoch.\n",
      " |              The model will not be trained on this data. Thus, note the fact\n",
      " |              that the validation loss of data provided using\n",
      " |              `validation_split` or `validation_data` is not affected by\n",
      " |              regularization layers like noise and dropout.\n",
      " |              `validation_data` will override `validation_split`.\n",
      " |              It can be:\n",
      " |              - A tuple `(x_val, y_val)` of NumPy arrays or tensors.\n",
      " |              - A tuple `(x_val, y_val, val_sample_weights)` of NumPy\n",
      " |              arrays.\n",
      " |              - A `keras.utils.PyDataset`, a `tf.data.Dataset`, a\n",
      " |              `torch.utils.data.DataLoader` yielding `(inputs, targets)` or a\n",
      " |              Python generator function yielding `(x_val, y_val)` or\n",
      " |              `(inputs, targets, sample_weights)`.\n",
      " |          shuffle: Boolean, whether to shuffle the training data before each\n",
      " |              epoch. This argument is ignored when `x` is a\n",
      " |              `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
      " |              `torch.utils.data.DataLoader` or Python generator function.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers)\n",
      " |              to a weight (float) value, used for weighting the loss function\n",
      " |              (during training only).\n",
      " |              This can be useful to tell the model to\n",
      " |              \"pay more attention\" to samples from\n",
      " |              an under-represented class. When `class_weight` is specified\n",
      " |              and targets have a rank of 2 or greater, either `y` must be\n",
      " |              one-hot encoded, or an explicit final dimension of `1` must\n",
      " |              be included for sparse class labels.\n",
      " |          sample_weight: Optional NumPy array or tensor of weights for\n",
      " |              the training samples, used for weighting the loss function\n",
      " |              (during training only). You can either pass a flat (1D)\n",
      " |              NumPy array or tensor with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples), or in the case of\n",
      " |              temporal data, you can pass a 2D NumPy array or tensor with\n",
      " |              shape `(samples, sequence_length)` to apply a different weight\n",
      " |              to every timestep of every sample.\n",
      " |              This argument is not supported when `x` is a\n",
      " |              `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
      " |              `torch.utils.data.DataLoader` or Python generator function.\n",
      " |              Instead, provide `sample_weights` as the third element of `x`.\n",
      " |              Note that sample weighting does not apply to metrics specified\n",
      " |              via the `metrics` argument in `compile()`. To apply sample\n",
      " |              weighting to your metrics, you can specify them via the\n",
      " |              `weighted_metrics` in `compile()` instead.\n",
      " |          initial_epoch: Integer.\n",
      " |              Epoch at which to start training\n",
      " |              (useful for resuming a previous training run).\n",
      " |          steps_per_epoch: Integer or `None`.\n",
      " |              Total number of steps (batches of samples) before declaring one\n",
      " |              epoch finished and starting the next epoch. When training with\n",
      " |              input tensors or NumPy arrays, the default `None` means that the\n",
      " |              value used is the number of samples in your dataset divided by\n",
      " |              the batch size, or 1 if that cannot be determined.\n",
      " |              If `x` is a `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
      " |              `torch.utils.data.DataLoader` or Python generator function, the\n",
      " |              epoch will run until the input dataset is exhausted. When\n",
      " |              passing an infinitely repeating dataset, you must specify the\n",
      " |              `steps_per_epoch` argument, otherwise the training will run\n",
      " |              indefinitely.\n",
      " |          validation_steps: Integer or `None`.\n",
      " |              Only relevant if `validation_data` is provided.\n",
      " |              Total number of steps (batches of samples) to draw before\n",
      " |              stopping when performing validation at the end of every epoch.\n",
      " |              If `validation_steps` is `None`, validation will run until the\n",
      " |              `validation_data` dataset is exhausted. In the case of an\n",
      " |              infinitely repeating dataset, it will run indefinitely. If\n",
      " |              `validation_steps` is specified and only part of the dataset\n",
      " |              is consumed, the evaluation will start from the beginning of the\n",
      " |              dataset at each epoch. This ensures that the same validation\n",
      " |              samples are used every time.\n",
      " |          validation_batch_size: Integer or `None`.\n",
      " |              Number of samples per validation batch.\n",
      " |              If unspecified, will default to `batch_size`.\n",
      " |              Do not specify the `validation_batch_size` if your data is a\n",
      " |              `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
      " |              `torch.utils.data.DataLoader` or Python generator function\n",
      " |              since they generate batches.\n",
      " |          validation_freq: Only relevant if validation data is provided.\n",
      " |              Specifies how many training epochs to run\n",
      " |              before a new validation run is performed,\n",
      " |              e.g. `validation_freq=2` runs validation every 2 epochs.\n",
      " |      \n",
      " |      Unpacking behavior for iterator-like inputs:\n",
      " |          A common pattern is to pass an iterator like object such as a\n",
      " |          `tf.data.Dataset` or a `keras.utils.PyDataset` to `fit()`,\n",
      " |          which will in fact yield not only features (`x`)\n",
      " |          but optionally targets (`y`) and sample weights (`sample_weight`).\n",
      " |          Keras requires that the output of such iterator-likes be\n",
      " |          unambiguous. The iterator should return a tuple\n",
      " |          of length 1, 2, or 3, where the optional second and third elements\n",
      " |          will be used for `y` and `sample_weight` respectively.\n",
      " |          Any other type provided will be wrapped in\n",
      " |          a length-one tuple, effectively treating everything as `x`. When\n",
      " |          yielding dicts, they should still adhere to the top-level tuple\n",
      " |          structure,\n",
      " |          e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
      " |          features, targets, and weights from the keys of a single dict.\n",
      " |          A notable unsupported data type is the `namedtuple`. The reason is\n",
      " |          that it behaves like both an ordered datatype (tuple) and a mapping\n",
      " |          datatype (dict). So given a namedtuple of the form:\n",
      " |          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
      " |          it is ambiguous whether to reverse the order of the elements when\n",
      " |          interpreting the value. Even worse is a tuple of the form:\n",
      " |          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
      " |          where it is unclear if the tuple was intended to be unpacked\n",
      " |          into `x`, `y`, and `sample_weight` or passed through\n",
      " |          as a single element to `x`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `History` object. Its `History.history` attribute is\n",
      " |          a record of training loss values and metrics values\n",
      " |          at successive epochs, as well as validation loss values\n",
      " |          and validation metrics values (if applicable).\n",
      " |  \n",
      " |  loss(self, y, y_pred, sample_weight=None)\n",
      " |  \n",
      " |  make_predict_function(self, force=False)\n",
      " |  \n",
      " |  make_test_function(self, force=False)\n",
      " |  \n",
      " |  make_train_function(self, force=False)\n",
      " |  \n",
      " |  predict(self, x, batch_size=None, verbose='auto', steps=None, callbacks=None)\n",
      " |      Generates output predictions for the input samples.\n",
      " |      \n",
      " |      Computation is done in batches. This method is designed for batch\n",
      " |      processing of large numbers of inputs. It is not intended for use inside\n",
      " |      of loops that iterate over your data and process small numbers of inputs\n",
      " |      at a time.\n",
      " |      \n",
      " |      For small numbers of inputs that fit in one batch,\n",
      " |      directly use `__call__()` for faster execution, e.g.,\n",
      " |      `model(x)`, or `model(x, training=False)` if you have layers such as\n",
      " |      `BatchNormalization` that behave differently during\n",
      " |      inference.\n",
      " |      \n",
      " |      Note: See [this FAQ entry](\n",
      " |      https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call)\n",
      " |      for more details about the difference between `Model` methods\n",
      " |      `predict()` and `__call__()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data. It can be:\n",
      " |              - A NumPy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |              - A backend-native tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |              - A dict mapping input names to the corresponding array/tensors,\n",
      " |              if the model has named inputs.\n",
      " |              - A `keras.utils.PyDataset`.\n",
      " |              - A `tf.data.Dataset`.\n",
      " |              - A `torch.utils.data.DataLoader`.\n",
      " |              - A Python generator function.\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per batch of computation.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |              Do not specify the `batch_size` if your input data `x` is a\n",
      " |              `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
      " |              `torch.utils.data.DataLoader` or Python generator function\n",
      " |              since they generate batches.\n",
      " |          verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = single line.\n",
      " |              `\"auto\"` becomes 1 for most cases. Note that the progress bar\n",
      " |              is not particularly useful when logged to a file,\n",
      " |              so `verbose=2` is recommended when not running interactively\n",
      " |              (e.g. in a production environment). Defaults to `\"auto\"`.\n",
      " |          steps: Total number of steps (batches of samples) to draw before\n",
      " |              declaring the prediction round finished. If `steps` is `None`,\n",
      " |              it will run until `x` is exhausted. In the case of an infinitely\n",
      " |              repeating dataset, it will run indefinitely.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during prediction.\n",
      " |      \n",
      " |      Returns:\n",
      " |          NumPy array(s) of predictions.\n",
      " |  \n",
      " |  predict_on_batch(self, x)\n",
      " |      Returns predictions for a single batch of samples.\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data. It must be array-like.\n",
      " |      \n",
      " |      Returns:\n",
      " |          NumPy array(s) of predictions.\n",
      " |  \n",
      " |  predict_step(self, data)\n",
      " |  \n",
      " |  test_on_batch(self, x, y=None, sample_weight=None, return_dict=False)\n",
      " |      Test the model on a single batch of samples.\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data. Must be array-like.\n",
      " |          y: Target data. Must be array-like.\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |              weights to apply to the model's loss for each sample.\n",
      " |              In the case of temporal data, you can pass a 2D array\n",
      " |              with shape `(samples, sequence_length)`, to apply a different\n",
      " |              weight to every timestep of every sample.\n",
      " |          return_dict: If `True`, loss and metric results are returned as a\n",
      " |              dict, with each key being the name of the metric. If `False`,\n",
      " |              they are returned as a list.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A scalar loss value (when no metrics and `return_dict=False`),\n",
      " |          a list of loss and metric values\n",
      " |          (if there are metrics and `return_dict=False`), or a dict of\n",
      " |          metric and loss values (if `return_dict=True`).\n",
      " |  \n",
      " |  test_step(self, data)\n",
      " |  \n",
      " |  train_on_batch(self, x, y=None, sample_weight=None, class_weight=None, return_dict=False)\n",
      " |      Runs a single gradient update on a single batch of data.\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data. Must be array-like.\n",
      " |          y: Target data. Must be array-like.\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |              weights to apply to the model's loss for each sample.\n",
      " |              In the case of temporal data, you can pass a 2D array\n",
      " |              with shape `(samples, sequence_length)`, to apply a different\n",
      " |              weight to every timestep of every sample.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers)\n",
      " |              to a weight (float) to apply to the model's loss for the samples\n",
      " |              from this class during training. This can be useful to tell the\n",
      " |              model to \"pay more attention\" to samples from an\n",
      " |              under-represented class. When `class_weight` is specified\n",
      " |              and targets have a rank of 2 or greater, either `y` must\n",
      " |              be one-hot encoded, or an explicit final dimension of 1\n",
      " |              must be included for sparse class labels.\n",
      " |          return_dict: If `True`, loss and metric results are returned as a\n",
      " |              dict, with each key being the name of the metric. If `False`,\n",
      " |              they are returned as a list.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A scalar loss value (when no metrics and `return_dict=False`),\n",
      " |          a list of loss and metric values\n",
      " |          (if there are metrics and `return_dict=False`), or a dict of\n",
      " |          metric and loss values (if `return_dict=True`).\n",
      " |  \n",
      " |  train_step(self, data)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.backend.tensorflow.trainer.TensorFlowTrainer:\n",
      " |  \n",
      " |  compiled_metrics\n",
      " |  \n",
      " |  distribute_strategy\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.src.backend.tensorflow.trainer.TensorFlowTrainer:\n",
      " |  \n",
      " |  distribute_reduction_method\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.trainers.trainer.Trainer:\n",
      " |  \n",
      " |  compile(self, optimizer='rmsprop', loss=None, loss_weights=None, metrics=None, weighted_metrics=None, run_eagerly=False, steps_per_execution=1, jit_compile='auto', auto_scale_loss=True)\n",
      " |      Configures the model for training.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      model.compile(\n",
      " |          optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
      " |          loss=keras.losses.BinaryCrossentropy(),\n",
      " |          metrics=[\n",
      " |              keras.metrics.BinaryAccuracy(),\n",
      " |              keras.metrics.FalseNegatives(),\n",
      " |          ],\n",
      " |      )\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          optimizer: String (name of optimizer) or optimizer instance. See\n",
      " |              `keras.optimizers`.\n",
      " |          loss: Loss function. May be a string (name of loss function), or\n",
      " |              a `keras.losses.Loss` instance. See `keras.losses`. A\n",
      " |              loss function is any callable with the signature\n",
      " |              `loss = fn(y_true, y_pred)`, where `y_true` are the ground truth\n",
      " |              values, and `y_pred` are the model's predictions.\n",
      " |              `y_true` should have shape `(batch_size, d0, .. dN)`\n",
      " |              (except in the case of sparse loss functions such as\n",
      " |              sparse categorical crossentropy which expects integer arrays of\n",
      " |              shape `(batch_size, d0, .. dN-1)`).\n",
      " |              `y_pred` should have shape `(batch_size, d0, .. dN)`.\n",
      " |              The loss function should return a float tensor.\n",
      " |          loss_weights: Optional list or dictionary specifying scalar\n",
      " |              coefficients (Python floats) to weight the loss contributions of\n",
      " |              different model outputs. The loss value that will be minimized\n",
      " |              by the model will then be the *weighted sum* of all individual\n",
      " |              losses, weighted by the `loss_weights` coefficients.  If a list,\n",
      " |              it is expected to have a 1:1 mapping to the model's outputs. If\n",
      " |              a dict, it is expected to map output names (strings) to scalar\n",
      " |              coefficients.\n",
      " |          metrics: List of metrics to be evaluated by the model during\n",
      " |              training and testing. Each of this can be a string (name of a\n",
      " |              built-in function), function or a `keras.metrics.Metric`\n",
      " |              instance. See `keras.metrics`. Typically you will use\n",
      " |              `metrics=['accuracy']`. A function is any callable with the\n",
      " |              signature `result = fn(y_true, _pred)`. To specify different\n",
      " |              metrics for different outputs of a multi-output model, you could\n",
      " |              also pass a dictionary, such as\n",
      " |              `metrics={'a':'accuracy', 'b':['accuracy', 'mse']}`.\n",
      " |              You can also pass a list to specify a metric or a list of\n",
      " |              metrics for each output, such as\n",
      " |              `metrics=[['accuracy'], ['accuracy', 'mse']]`\n",
      " |              or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass\n",
      " |              the strings 'accuracy' or 'acc', we convert this to one of\n",
      " |              `keras.metrics.BinaryAccuracy`,\n",
      " |              `keras.metrics.CategoricalAccuracy`,\n",
      " |              `keras.metrics.SparseCategoricalAccuracy` based on the\n",
      " |              shapes of the targets and of the model output. A similar\n",
      " |              conversion is done for the strings `\"crossentropy\"`\n",
      " |              and `\"ce\"` as well.\n",
      " |              The metrics passed here are evaluated without sample weighting;\n",
      " |              if you would like sample weighting to apply, you can specify\n",
      " |              your metrics via the `weighted_metrics` argument instead.\n",
      " |          weighted_metrics: List of metrics to be evaluated and weighted by\n",
      " |              `sample_weight` or `class_weight` during training and testing.\n",
      " |          run_eagerly: Bool. If `True`, this model's forward pass\n",
      " |               will never be compiled. It is recommended to leave this\n",
      " |               as `False` when training (for best performance),\n",
      " |               and to set it to `True` when debugging.\n",
      " |          steps_per_execution: Int. The number of batches to run\n",
      " |              during each a single compiled function call. Running multiple\n",
      " |              batches inside a single compiled function call can\n",
      " |              greatly improve performance on TPUs or small models with a large\n",
      " |              Python overhead. At most, one full epoch will be run each\n",
      " |              execution. If a number larger than the size of the epoch is\n",
      " |              passed, the execution will be truncated to the size of the\n",
      " |              epoch. Note that if `steps_per_execution` is set to `N`,\n",
      " |              `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n",
      " |              will only be called every `N` batches (i.e. before/after\n",
      " |              each compiled function execution).\n",
      " |              Not supported with the PyTorch backend.\n",
      " |          jit_compile: Bool or `\"auto\"`. Whether to use XLA compilation when\n",
      " |              compiling a model. For `jax` and `tensorflow` backends,\n",
      " |              `jit_compile=\"auto\"` enables XLA compilation if the model\n",
      " |              supports it, and disabled otherwise.\n",
      " |              For `torch` backend, `\"auto\"` will default to eager\n",
      " |              execution and `jit_compile=True` will run with `torch.compile`\n",
      " |              with the `\"inductor\"` backend.\n",
      " |          auto_scale_loss: Bool. If `True` and the model dtype policy is\n",
      " |              `\"mixed_float16\"`, the passed optimizer will be automatically\n",
      " |              wrapped in a `LossScaleOptimizer`, which will dynamically\n",
      " |              scale the loss to prevent underflow.\n",
      " |  \n",
      " |  compile_from_config(self, config)\n",
      " |      Compiles the model with the information given in config.\n",
      " |      \n",
      " |      This method uses the information in the config (optimizer, loss,\n",
      " |      metrics, etc.) to compile the model.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: Dict containing information for compiling the model.\n",
      " |  \n",
      " |  compute_loss(self, x=None, y=None, y_pred=None, sample_weight=None, training=True)\n",
      " |      Compute the total loss, validate it, and return it.\n",
      " |      \n",
      " |      Subclasses can optionally override this method to provide custom loss\n",
      " |      computation logic.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyModel(Model):\n",
      " |          def __init__(self, *args, **kwargs):\n",
      " |              super().__init__(*args, **kwargs)\n",
      " |              self.loss_tracker = metrics.Mean(name='loss')\n",
      " |      \n",
      " |          def compute_loss(self, x, y, y_pred, sample_weight, training=True):\n",
      " |              loss = ops.mean((y_pred - y) ** 2)\n",
      " |              loss += ops.sum(self.losses)\n",
      " |              self.loss_tracker.update_state(loss)\n",
      " |              return loss\n",
      " |      \n",
      " |          def reset_metrics(self):\n",
      " |              self.loss_tracker.reset_state()\n",
      " |      \n",
      " |          @property\n",
      " |          def metrics(self):\n",
      " |              return [self.loss_tracker]\n",
      " |      \n",
      " |      inputs = layers.Input(shape=(10,), name='my_input')\n",
      " |      outputs = layers.Dense(10)(inputs)\n",
      " |      model = MyModel(inputs, outputs)\n",
      " |      model.add_loss(ops.sum(outputs))\n",
      " |      \n",
      " |      optimizer = SGD()\n",
      " |      model.compile(optimizer, loss='mse', steps_per_execution=10)\n",
      " |      dataset = ...\n",
      " |      model.fit(dataset, epochs=2, steps_per_epoch=10)\n",
      " |      print(f\"Custom loss: {model.loss_tracker.result()}\")\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data.\n",
      " |          y: Target data.\n",
      " |          y_pred: Predictions returned by the model (output of `model(x)`)\n",
      " |          sample_weight: Sample weights for weighting the loss function.\n",
      " |          training: Whether we are training or evaluating the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The total loss as a scalar tensor, or `None` if no loss results\n",
      " |          (which is the case when called by `Model.test_step`).\n",
      " |  \n",
      " |  compute_metrics(self, x, y, y_pred, sample_weight=None)\n",
      " |      Update metric states and collect all metrics to be returned.\n",
      " |      \n",
      " |      Subclasses can optionally override this method to provide custom metric\n",
      " |      updating and collection logic. Custom metrics are not passed in\n",
      " |      `compile()`, they can be created in `__init__` or `build`. They are\n",
      " |      automatically tracked and returned by `self.metrics`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyModel(Sequential):\n",
      " |          def __init__(self, *args, **kwargs):\n",
      " |              super().__init__(*args, **kwargs)\n",
      " |              self.custom_metric = MyMetric(name=\"custom_metric\")\n",
      " |      \n",
      " |          def compute_metrics(self, x, y, y_pred, sample_weight):\n",
      " |              # This super call updates metrics from `compile` and returns\n",
      " |              # results for all metrics listed in `self.metrics`.\n",
      " |              metric_results = super().compute_metrics(\n",
      " |                  x, y, y_pred, sample_weight)\n",
      " |      \n",
      " |              # `metric_results` contains the previous result for\n",
      " |              # `custom_metric`, this is where we update it.\n",
      " |              self.custom_metric.update_state(x, y, y_pred, sample_weight)\n",
      " |              metric_results['custom_metric'] = self.custom_metric.result()\n",
      " |              return metric_results\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data.\n",
      " |          y: Target data.\n",
      " |          y_pred: Predictions returned by the model output of `model.call(x)`.\n",
      " |          sample_weight: Sample weights for weighting the loss function.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `dict` containing values that will be passed to\n",
      " |          `keras.callbacks.CallbackList.on_train_batch_end()`. Typically,\n",
      " |          the values of the metrics listed in `self.metrics` are returned.\n",
      " |          Example: `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  get_compile_config(self)\n",
      " |      Returns a serialized config with information for compiling the model.\n",
      " |      \n",
      " |      This method returns a config dictionary containing all the information\n",
      " |      (optimizer, loss, metrics, etc.) with which the model was compiled.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict containing information for compiling the model.\n",
      " |  \n",
      " |  get_metrics_result(self)\n",
      " |      Returns the model's metrics values as a dict.\n",
      " |      \n",
      " |      If any of the metric result is a dict (containing multiple metrics),\n",
      " |      each of them gets added to the top level returned dict of this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `dict` containing values of the metrics listed in `self.metrics`.\n",
      " |          Example: `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  reset_metrics(self)\n",
      " |  \n",
      " |  stateless_compute_loss(self, trainable_variables, non_trainable_variables, metrics_variables, x=None, y=None, y_pred=None, sample_weight=None, training=True)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.trainers.trainer.Trainer:\n",
      " |  \n",
      " |  metrics\n",
      " |  \n",
      " |  metrics_names\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.src.trainers.trainer.Trainer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  jit_compile\n",
      " |  \n",
      " |  run_eagerly\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.layers.layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_loss(self, loss)\n",
      " |      Can be called inside of the `call()` method to add a scalar loss.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(Layer):\n",
      " |          ...\n",
      " |          def call(self, x):\n",
      " |              self.add_loss(ops.sum(x))\n",
      " |              return x\n",
      " |      ```\n",
      " |  \n",
      " |  add_metric(self, *args, **kwargs)\n",
      " |  \n",
      " |  add_variable(self, shape, initializer, dtype=None, trainable=True, autocast=True, regularizer=None, constraint=None, name=None)\n",
      " |      Add a weight variable to the layer.\n",
      " |      \n",
      " |      Alias of `add_weight()`.\n",
      " |  \n",
      " |  add_weight(self, shape=None, initializer=None, dtype=None, trainable=True, autocast=True, regularizer=None, constraint=None, aggregation='none', name=None)\n",
      " |      Add a weight variable to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |          shape: Shape tuple for the variable. Must be fully-defined\n",
      " |              (no `None` entries). Defaults to `()` (scalar) if unspecified.\n",
      " |          initializer: Initializer object to use to populate the initial\n",
      " |              variable value, or string name of a built-in initializer\n",
      " |              (e.g. `\"random_normal\"`). If unspecified, defaults to\n",
      " |              `\"glorot_uniform\"` for floating-point variables and to `\"zeros\"`\n",
      " |              for all other types (e.g. int, bool).\n",
      " |          dtype: Dtype of the variable to create, e.g. `\"float32\"`. If\n",
      " |              unspecified, defaults to the layer's variable dtype\n",
      " |              (which itself defaults to `\"float32\"` if unspecified).\n",
      " |          trainable: Boolean, whether the variable should be trainable via\n",
      " |              backprop or whether its updates are managed manually. Defaults\n",
      " |              to `True`.\n",
      " |          autocast: Boolean, whether to autocast layers variables when\n",
      " |              accessing them. Defaults to `True`.\n",
      " |          regularizer: Regularizer object to call to apply penalty on the\n",
      " |              weight. These penalties are summed into the loss function\n",
      " |              during optimization. Defaults to `None`.\n",
      " |          constraint: Contrainst object to call on the variable after any\n",
      " |              optimizer update, or string name of a built-in constraint.\n",
      " |              Defaults to `None`.\n",
      " |          aggregation: Optional string, one of `None`, `\"none\"`, `\"mean\"`,\n",
      " |              `\"sum\"` or `\"only_first_replica\"`. Annotates the variable with\n",
      " |              the type of multi-replica aggregation to be used for this\n",
      " |              variable when writing custom data parallel training loops.\n",
      " |              Defaults to `\"none\"`.\n",
      " |          name: String name of the variable. Useful for debugging purposes.\n",
      " |  \n",
      " |  compute_mask(self, inputs, previous_mask)\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |  \n",
      " |  get_build_config(self)\n",
      " |      Returns a dictionary with the layer's input shape.\n",
      " |      \n",
      " |      This method returns a config dict that can be used by\n",
      " |      `build_from_config(config)` to create all states (e.g. Variables and\n",
      " |      Lookup tables) needed by the layer.\n",
      " |      \n",
      " |      By default, the config only contains the input shape that the layer\n",
      " |      was built with. If you're writing a custom layer that creates state in\n",
      " |      an unusual way, you should override this method to make sure this state\n",
      " |      is already created when Keras attempts to load its value upon model\n",
      " |      loading.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict containing the input shape associated with the layer.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Return the values of `layer.weights` as a list of NumPy arrays.\n",
      " |  \n",
      " |  load_own_variables(self, store)\n",
      " |      Loads the state of the layer.\n",
      " |      \n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is loaded upon calling `keras.models.load_model()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          store: Dict from which the state of the model will be loaded.\n",
      " |  \n",
      " |  quantized_build(self, input_shape, mode)\n",
      " |  \n",
      " |  quantized_call(self, *args, **kwargs)\n",
      " |  \n",
      " |  save_own_variables(self, store)\n",
      " |      Saves the state of the layer.\n",
      " |      \n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is saved upon calling `model.save()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          store: Dict where the state of the model will be saved.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the values of `layer.weights` from a list of NumPy arrays.\n",
      " |  \n",
      " |  stateless_call(self, trainable_variables, non_trainable_variables, *args, return_losses=False, **kwargs)\n",
      " |      Call the layer without any side effects.\n",
      " |      \n",
      " |      Args:\n",
      " |          trainable_variables: List of trainable variables of the model.\n",
      " |          non_trainable_variables: List of non-trainable variables of the\n",
      " |              model.\n",
      " |          *args: Positional arguments to be passed to `call()`.\n",
      " |          return_losses: If `True`, `stateless_call()` will return the list of\n",
      " |              losses created during `call()` as part of its return values.\n",
      " |          **kwargs: Keyword arguments to be passed to `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple. By default, returns `(outputs, non_trainable_variables)`.\n",
      " |              If `return_losses = True`, then returns\n",
      " |              `(outputs, non_trainable_variables, losses)`.\n",
      " |      \n",
      " |      Note: `non_trainable_variables` include not only non-trainable weights\n",
      " |      such as `BatchNormalization` statistics, but also RNG seed state\n",
      " |      (if there are any random operations part of the layer, such as dropout),\n",
      " |      and `Metric` state (if there are any metrics attached to the layer).\n",
      " |      These are all elements of state of the layer.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      model = ...\n",
      " |      data = ...\n",
      " |      trainable_variables = model.trainable_variables\n",
      " |      non_trainable_variables = model.non_trainable_variables\n",
      " |      # Call the model with zero side effects\n",
      " |      outputs, non_trainable_variables = model.stateless_call(\n",
      " |          trainable_variables,\n",
      " |          non_trainable_variables,\n",
      " |          data,\n",
      " |      )\n",
      " |      # Attach the updated state to the model\n",
      " |      # (until you do this, the model is still in its pre-call state).\n",
      " |      for ref_var, value in zip(\n",
      " |          model.non_trainable_variables, non_trainable_variables\n",
      " |      ):\n",
      " |          ref_var.assign(value)\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.layers.layer.Layer:\n",
      " |  \n",
      " |  compute_dtype\n",
      " |      The dtype of the computations performed by the layer.\n",
      " |  \n",
      " |  dtype\n",
      " |      Alias of `layer.variable_dtype`.\n",
      " |  \n",
      " |  losses\n",
      " |      List of scalar losses from `add_loss`, regularizers and sublayers.\n",
      " |  \n",
      " |  metrics_variables\n",
      " |      List of all metric variables.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |      List of all non-trainable layer state.\n",
      " |      \n",
      " |      This extends `layer.non_trainable_weights` to include all state used by\n",
      " |      the layer including state for metrics and `SeedGenerator`s.\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weight variables of the layer.\n",
      " |      \n",
      " |      These are the weights that should not be updated by the optimizer during\n",
      " |      training. Unlike, `layer.non_trainable_variables` this excludes metric\n",
      " |      state and random seeds.\n",
      " |  \n",
      " |  path\n",
      " |      The path of the layer.\n",
      " |      \n",
      " |      If the layer has not been built yet, it will be `None`.\n",
      " |  \n",
      " |  quantization_mode\n",
      " |      The quantization mode of this layer, `None` if not quantized.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      List of all trainable layer state.\n",
      " |      \n",
      " |      This is equivalent to `layer.trainable_weights`.\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weight variables of the layer.\n",
      " |      \n",
      " |      These are the weights that get updated by the optimizer during training.\n",
      " |  \n",
      " |  variable_dtype\n",
      " |      The dtype of the state (weights) of the layer.\n",
      " |  \n",
      " |  variables\n",
      " |      List of all layer state, including random seeds.\n",
      " |      \n",
      " |      This extends `layer.weights` to include all state used by the layer\n",
      " |      including `SeedGenerator`s.\n",
      " |      \n",
      " |      Note that metrics variables are not included here, use\n",
      " |      `metrics_variables` to visit all the metric variables.\n",
      " |  \n",
      " |  weights\n",
      " |      List of all weight variables of the layer.\n",
      " |      \n",
      " |      Unlike, `layer.variables` this excludes metric state and random seeds.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.src.layers.layer.Layer:\n",
      " |  \n",
      " |  dtype_policy\n",
      " |  \n",
      " |  input_spec\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |      Settable boolean, whether this layer should be trainable or not.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.ops.operation.Operation:\n",
      " |  \n",
      " |  symbolic_call(self, *args, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.ops.operation.Operation:\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a symbolic operation.\n",
      " |      \n",
      " |      Only returns the tensor(s) corresponding to the *first time*\n",
      " |      the operation was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only returns the tensor(s) corresponding to the *first time*\n",
      " |      the operation was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output tensor or list of output tensors.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.saving.keras_saveable.KerasSaveable:\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      __reduce__ is used to customize the behavior of `pickle.pickle()`.\n",
      " |      \n",
      " |      The method returns a tuple of two elements: a function, and a list of\n",
      " |      arguments to pass to that function.  In this case we just leverage the\n",
      " |      keras saving library.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export_path = /var/folders/hs/y99phmj97056_13dh09cgk1c0000gn/T/1\n",
      "INFO:tensorflow:Assets written to: /var/folders/hs/y99phmj97056_13dh09cgk1c0000gn/T/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/hs/y99phmj97056_13dh09cgk1c0000gn/T/1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/hs/y99phmj97056_13dh09cgk1c0000gn/T/1'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  5395780528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5393388992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5395058176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5395055888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Saved model:\n",
      "total 80\n",
      "drwxr-xr-x  2 dgmr2  staff     64 Jan 15 21:08 \u001b[34massets\u001b[m\u001b[m\n",
      "-rw-r--r--  1 dgmr2  staff     59 Jan 15 21:08 fingerprint.pb\n",
      "-rw-r--r--  1 dgmr2  staff  34669 Jan 15 21:08 saved_model.pb\n",
      "drwxr-xr-x  4 dgmr2  staff    128 Jan 15 21:08 \u001b[34mvariables\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = tempfile.gettempdir()\n",
    "version = 1\n",
    "export_path = os.path.join(MODEL_DIR, str(version))\n",
    "print(f'export_path = {export_path}')\n",
    "\n",
    "model.export(export_path, format='tf_saved_model', verbose=True)\n",
    "\n",
    "print('Saved model:')\n",
    "!ls -l {export_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(idx, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(test_images[idx].reshape(28, 28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'{title}')\n",
    "\n",
    "import json\n",
    "data = json.dumps({'signature_name':'serving_default', 'instances': test_images[0:3].tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"signature_name\": \"serving_default\", \"instances\": [[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.011764705882352941], [0.00392156862745098], [0.0], [0.0], [0.027450980392156862], [0.0], [0.1450980392156863], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.00784313725490196], [0.0], [0.10588235294117647], [0.32941176470588235], [0.043137254901960784], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.4666666666666667], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.34509803921568627], [0.5607843137254902], [0.43137254901960786], [0.0], [0.0], [0.0], [0.0], [0.08627450980392157], [0.36470588235294116], [0.41568627450980394], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.01568627450980392], [0.0], [0.20784313725490197], [0.5058823529411764], [0.47058823529411764], [0.5764705882352941], [0.6862745098039216], [0.615686274509804], [0.6509803921568628], [0.5294117647058824], [0.6039215686274509], [0.6588235294117647], [0.5490196078431373], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00784313725490196], [0.0], [0.043137254901960784], [0.5372549019607843], [0.5098039215686274], [0.5019607843137255], [0.6274509803921569], [0.6901960784313725], [0.6235294117647059], [0.6549019607843137], [0.6980392156862745], [0.5843137254901961], [0.592156862745098], [0.5647058823529412], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.00784313725490196], [0.00392156862745098], [0.0], [0.011764705882352941], [0.0], [0.0], [0.45098039215686275], [0.4470588235294118], [0.41568627450980394], [0.5372549019607843], [0.6588235294117647], [0.6], [0.611764705882353], [0.6470588235294118], [0.6549019607843137], [0.5607843137254902], [0.615686274509804], [0.6196078431372549], [0.043137254901960784], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.0], [0.0], [0.0], [0.011764705882352941], [0.0], [0.0], [0.34901960784313724], [0.5450980392156862], [0.35294117647058826], [0.3686274509803922], [0.6], [0.5843137254901961], [0.5137254901960784], [0.592156862745098], [0.6627450980392157], [0.6745098039215687], [0.5607843137254902], [0.6235294117647059], [0.6627450980392157], [0.18823529411764706], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00784313725490196], [0.01568627450980392], [0.00392156862745098], [0.0], [0.0], [0.0], [0.3843137254901961], [0.5333333333333333], [0.43137254901960786], [0.42745098039215684], [0.43137254901960786], [0.6352941176470588], [0.5294117647058824], [0.5647058823529412], [0.5843137254901961], [0.6235294117647059], [0.6549019607843137], [0.5647058823529412], [0.6196078431372549], [0.6627450980392157], [0.4666666666666667], [0.0]], [[0.0], [0.0], [0.00784313725490196], [0.00784313725490196], [0.00392156862745098], [0.00784313725490196], [0.0], [0.0], [0.0], [0.0], [0.10196078431372549], [0.4235294117647059], [0.4588235294117647], [0.38823529411764707], [0.43529411764705883], [0.4588235294117647], [0.5333333333333333], [0.611764705882353], [0.5254901960784314], [0.6039215686274509], [0.6039215686274509], [0.611764705882353], [0.6274509803921569], [0.5529411764705883], [0.5764705882352941], [0.611764705882353], [0.6980392156862745], [0.0]], [[0.011764705882352941], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.08235294117647059], [0.20784313725490197], [0.3607843137254902], [0.4588235294117647], [0.43529411764705883], [0.403921568627451], [0.45098039215686275], [0.5058823529411764], [0.5254901960784314], [0.5607843137254902], [0.6039215686274509], [0.6470588235294118], [0.6666666666666666], [0.6039215686274509], [0.592156862745098], [0.6039215686274509], [0.5607843137254902], [0.5411764705882353], [0.5882352941176471], [0.6470588235294118], [0.16862745098039217]], [[0.0], [0.0], [0.09019607843137255], [0.21176470588235294], [0.2549019607843137], [0.2980392156862745], [0.3333333333333333], [0.4627450980392157], [0.5019607843137255], [0.4823529411764706], [0.43529411764705883], [0.44313725490196076], [0.4627450980392157], [0.4980392156862745], [0.49019607843137253], [0.5450980392156862], [0.5215686274509804], [0.5333333333333333], [0.6274509803921569], [0.5490196078431373], [0.6078431372549019], [0.6313725490196078], [0.5647058823529412], [0.6078431372549019], [0.6745098039215687], [0.6313725490196078], [0.7411764705882353], [0.24313725490196078]], [[0.0], [0.26666666666666666], [0.3686274509803922], [0.35294117647058826], [0.43529411764705883], [0.4470588235294118], [0.43529411764705883], [0.4470588235294118], [0.45098039215686275], [0.4980392156862745], [0.5294117647058824], [0.5333333333333333], [0.5607843137254902], [0.49411764705882355], [0.4980392156862745], [0.592156862745098], [0.6039215686274509], [0.5607843137254902], [0.5803921568627451], [0.49019607843137253], [0.6352941176470588], [0.6352941176470588], [0.5647058823529412], [0.5411764705882353], [0.6], [0.6352941176470588], [0.7686274509803922], [0.22745098039215686]], [[0.27450980392156865], [0.6627450980392157], [0.5058823529411764], [0.40784313725490196], [0.3843137254901961], [0.39215686274509803], [0.3686274509803922], [0.3803921568627451], [0.3843137254901961], [0.4], [0.4235294117647059], [0.41568627450980394], [0.4666666666666667], [0.47058823529411764], [0.5058823529411764], [0.5843137254901961], [0.611764705882353], [0.6549019607843137], [0.7450980392156863], [0.7450980392156863], [0.7686274509803922], [0.7764705882352941], [0.7764705882352941], [0.7333333333333333], [0.7725490196078432], [0.7411764705882353], [0.7215686274509804], [0.1411764705882353]], [[0.06274509803921569], [0.49411764705882355], [0.6705882352941176], [0.7372549019607844], [0.7372549019607844], [0.7215686274509804], [0.6705882352941176], [0.6], [0.5294117647058824], [0.47058823529411764], [0.49411764705882355], [0.4980392156862745], [0.5725490196078431], [0.7254901960784313], [0.7647058823529411], [0.8196078431372549], [0.8156862745098039], [1.0], [0.8196078431372549], [0.6941176470588235], [0.9607843137254902], [0.9882352941176471], [0.984313725490196], [0.984313725490196], [0.9686274509803922], [0.8627450980392157], [0.807843137254902], [0.19215686274509805]], [[0.0], [0.0], [0.0], [0.047058823529411764], [0.2627450980392157], [0.41568627450980394], [0.6431372549019608], [0.7254901960784313], [0.7803921568627451], [0.8235294117647058], [0.8274509803921568], [0.8235294117647058], [0.8156862745098039], [0.7450980392156863], [0.5882352941176471], [0.3215686274509804], [0.03137254901960784], [0.0], [0.0], [0.0], [0.6980392156862745], [0.8156862745098039], [0.7372549019607844], [0.6862745098039216], [0.6352941176470588], [0.6196078431372549], [0.592156862745098], [0.043137254901960784]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]], [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.050980392156862744], [0.2627450980392157], [0.0], [0.0], [0.0], [0.0], [0.19607843137254902], [0.14901960784313725], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.03137254901960784], [0.47058823529411764], [0.8196078431372549], [0.8862745098039215], [0.9686274509803922], [0.9294117647058824], [1.0], [1.0], [1.0], [0.9686274509803922], [0.9333333333333333], [0.9215686274509803], [0.6745098039215687], [0.2823529411764706], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.5372549019607843], [0.9372549019607843], [0.9882352941176471], [0.9529411764705882], [0.9176470588235294], [0.8980392156862745], [0.9333333333333333], [0.9568627450980393], [0.9647058823529412], [0.9411764705882353], [0.9019607843137255], [0.9098039215686274], [0.9372549019607843], [0.9725490196078431], [0.984313725490196], [0.7607843137254902], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.4], [1.0], [0.9058823529411765], [0.8941176470588236], [0.8901960784313725], [0.8941176470588236], [0.9137254901960784], [0.9019607843137255], [0.9019607843137255], [0.8980392156862745], [0.8941176470588236], [0.9098039215686274], [0.9098039215686274], [0.9058823529411765], [0.8901960784313725], [0.8784313725490196], [0.9882352941176471], [0.7019607843137254], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.9137254901960784], [0.9450980392156862], [0.8980392156862745], [0.9058823529411765], [1.0], [1.0], [0.9333333333333333], [0.9058823529411765], [0.8901960784313725], [0.9333333333333333], [0.9647058823529412], [0.8941176470588236], [0.9019607843137255], [0.8901960784313725], [0.9176470588235294], [0.9215686274509803], [0.8980392156862745], [0.9450980392156862], [0.0784313725490196], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.9725490196078431], [0.9450980392156862], [0.9058823529411765], [1.0], [0.5843137254901961], [0.1843137254901961], [0.9882352941176471], [0.8941176470588236], [1.0], [0.9490196078431372], [0.8470588235294118], [0.9333333333333333], [0.9098039215686274], [1.0], [0.8941176470588236], [0.8627450980392157], [0.9176470588235294], [0.9803921568627451], [0.21176470588235294], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9411764705882353], [0.9098039215686274], [1.0], [0.058823529411764705], [0.0], [1.0], [0.9294117647058824], [0.7490196078431373], [0.0], [0.0], [0.8392156862745098], [1.0], [0.050980392156862744], [0.4823529411764706], [1.0], [0.9176470588235294], [0.9882352941176471], [0.4470588235294118], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.023529411764705882], [1.0], [0.9333333333333333], [0.9372549019607843], [1.0], [0.6941176470588235], [0.0], [1.0], [1.0], [0.0], [0.5098039215686274], [0.4549019607843137], [0.1843137254901961], [0.2549019607843137], [0.16862745098039217], [0.1450980392156863], [1.0], [0.9254901960784314], [0.9764705882352941], [0.6352941176470588], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.12549019607843137], [1.0], [0.9254901960784314], [0.9607843137254902], [1.0], [0.8], [0.0], [1.0], [0.32941176470588235], [0.0], [0.1450980392156863], [0.10980392156862745], [0.12156862745098039], [0.0], [0.09803921568627451], [0.050980392156862744], [1.0], [0.9254901960784314], [0.9764705882352941], [0.7803921568627451], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.20784313725490197], [1.0], [0.9254901960784314], [0.9803921568627451], [0.9803921568627451], [0.9058823529411765], [0.00784313725490196], [1.0], [0.08235294117647059], [0.0], [0.8666666666666667], [1.0], [0.9254901960784314], [0.21176470588235294], [0.9607843137254902], [0.7764705882352941], [0.9529411764705882], [0.9333333333333333], [0.9607843137254902], [0.8745098039215686], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.3137254901960784], [1.0], [0.9294117647058824], [0.9803921568627451], [0.9411764705882353], [1.0], [0.0], [0.0], [0.15294117647058825], [0.615686274509804], [0.0], [0.0], [0.8431372549019608], [0.3686274509803922], [0.0784313725490196], [0.49411764705882355], [1.0], [0.9294117647058824], [0.9372549019607843], [0.9803921568627451], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.396078431372549], [1.0], [0.9215686274509803], [0.9921568627450981], [0.9568627450980393], [0.9529411764705882], [0.5215686274509804], [0.5411764705882353], [0.8156862745098039], [1.0], [0.788235294117647], [0.8392156862745098], [1.0], [0.9019607843137255], [0.027450980392156862], [0.6823529411764706], [1.0], [0.9411764705882353], [0.9333333333333333], [1.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.49411764705882355], [1.0], [0.9137254901960784], [1.0], [0.9725490196078431], [0.9137254901960784], [1.0], [1.0], [0.9411764705882353], [0.9098039215686274], [0.9529411764705882], [0.9529411764705882], [0.9058823529411765], [0.984313725490196], [1.0], [1.0], [0.996078431372549], [0.9529411764705882], [0.9333333333333333], [1.0], [0.011764705882352941], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.5764705882352941], [1.0], [0.9137254901960784], [0.9764705882352941], [0.7098039215686275], [0.9529411764705882], [0.8901960784313725], [0.8784313725490196], [0.9019607843137255], [0.9176470588235294], [0.9019607843137255], [0.9019607843137255], [0.9215686274509803], [0.8941176470588236], [0.9215686274509803], [0.8705882352941177], [0.8117647058823529], [1.0], [0.9254901960784314], [1.0], [0.13725490196078433], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.6392156862745098], [1.0], [0.9607843137254902], [0.8666666666666667], [0.33725490196078434], [1.0], [0.9137254901960784], [0.9137254901960784], [0.9215686274509803], [0.9254901960784314], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9098039215686274], [0.9490196078431372], [0.9058823529411765], [0.49019607843137253], [1.0], [0.9254901960784314], [1.0], [0.21568627450980393], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7098039215686275], [0.996078431372549], [1.0], [0.7843137254901961], [0.27058823529411763], [1.0], [0.8941176470588236], [0.9098039215686274], [0.9176470588235294], [0.9215686274509803], [0.9176470588235294], [0.9176470588235294], [0.9137254901960784], [0.9215686274509803], [0.9450980392156862], [0.9294117647058824], [0.27450980392156865], [1.0], [0.9215686274509803], [0.9647058823529412], [0.2235294117647059], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7725490196078432], [0.9686274509803922], [1.0], [0.7372549019607844], [0.43137254901960786], [1.0], [0.8784313725490196], [0.9137254901960784], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9411764705882353], [0.9921568627450981], [0.27058823529411763], [1.0], [0.9254901960784314], [0.9725490196078431], [0.30196078431372547], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7843137254901961], [0.9647058823529412], [1.0], [0.5843137254901961], [0.5686274509803921], [1.0], [0.8745098039215686], [0.9215686274509803], [0.9176470588235294], [0.9215686274509803], [0.9215686274509803], [0.9215686274509803], [0.9176470588235294], [0.9294117647058824], [0.9137254901960784], [1.0], [0.1843137254901961], [1.0], [0.9372549019607843], [0.9764705882352941], [0.3843137254901961], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.8], [0.9529411764705882], [1.0], [0.43529411764705883], [0.6784313725490196], [1.0], [0.8901960784313725], [0.9215686274509803], [0.9215686274509803], [0.9254901960784314], [0.9215686274509803], [0.9215686274509803], [0.9215686274509803], [0.9372549019607843], [0.8980392156862745], [1.0], [0.07450980392156863], [0.8901960784313725], [0.9647058823529412], [0.9764705882352941], [0.43137254901960786], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7686274509803922], [0.9411764705882353], [1.0], [0.42745098039215684], [0.8352941176470589], [0.9803921568627451], [0.8980392156862745], [0.9215686274509803], [0.9215686274509803], [0.9254901960784314], [0.9215686274509803], [0.9294117647058824], [0.9254901960784314], [0.9294117647058824], [0.8862745098039215], [1.0], [0.21568627450980393], [0.796078431372549], [0.984313725490196], [0.9607843137254902], [0.47058823529411764], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7529411764705882], [0.9529411764705882], [1.0], [0.4470588235294118], [0.9098039215686274], [0.9411764705882353], [0.9098039215686274], [0.9215686274509803], [0.9215686274509803], [0.9254901960784314], [0.9176470588235294], [0.9294117647058824], [0.9254901960784314], [0.9215686274509803], [0.8980392156862745], [1.0], [0.5254901960784314], [0.6705882352941176], [0.9882352941176471], [0.9568627450980393], [0.5372549019607843], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7411764705882353], [0.984313725490196], [1.0], [0.6039215686274509], [0.9333333333333333], [0.9137254901960784], [0.9254901960784314], [0.9176470588235294], [0.9215686274509803], [0.9254901960784314], [0.9215686274509803], [0.9333333333333333], [0.9254901960784314], [0.9215686274509803], [0.9098039215686274], [1.0], [0.6509803921568628], [0.49019607843137253], [1.0], [0.9529411764705882], [0.5568627450980392], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7176470588235294], [0.9882352941176471], [1.0], [0.6705882352941176], [0.9686274509803922], [0.9098039215686274], [0.9176470588235294], [0.9176470588235294], [0.9137254901960784], [0.9137254901960784], [0.9098039215686274], [0.9176470588235294], [0.9137254901960784], [0.9176470588235294], [0.9137254901960784], [0.9411764705882353], [0.8745098039215686], [0.5019607843137255], [1.0], [0.9490196078431372], [0.592156862745098], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.6980392156862745], [0.9529411764705882], [1.0], [0.2235294117647059], [0.9333333333333333], [0.9450980392156862], [0.9333333333333333], [0.9333333333333333], [0.9333333333333333], [0.9294117647058824], [0.9254901960784314], [0.9294117647058824], [0.9294117647058824], [0.9411764705882353], [0.9294117647058824], [0.996078431372549], [0.6901960784313725], [0.20392156862745098], [1.0], [0.9372549019607843], [0.615686274509804], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7372549019607844], [0.9411764705882353], [0.9803921568627451], [0.24313725490196078], [0.8549019607843137], [1.0], [0.8627450980392157], [0.8705882352941177], [0.8705882352941177], [0.8705882352941177], [0.8745098039215686], [0.8745098039215686], [0.8784313725490196], [0.8705882352941177], [0.8549019607843137], [1.0], [0.6039215686274509], [0.12549019607843137], [1.0], [0.9254901960784314], [0.7372549019607844], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.5098039215686274], [0.9607843137254902], [0.9490196078431372], [0.09411764705882353], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.13333333333333333], [0.9490196078431372], [0.9568627450980393], [0.5294117647058824], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.2980392156862745], [1.0], [0.9764705882352941], [0.08627450980392157], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.15294117647058825], [0.9764705882352941], [1.0], [0.4823529411764706], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.19215686274509805], [0.803921568627451], [0.7725490196078432], [0.043137254901960784], [0.0], [0.01568627450980392], [0.00392156862745098], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.011764705882352941], [0.0], [0.011764705882352941], [0.6823529411764706], [0.7411764705882353], [0.2627450980392157], [0.0], [0.0], [0.0]]], [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.2627450980392157], [0.6941176470588235], [0.5058823529411764], [0.6], [0.4588235294117647], [0.5058823529411764], [0.5725490196078431], [0.5529411764705883], [0.6862745098039216], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00784313725490196], [0.0], [0.7686274509803922], [1.0], [1.0], [1.0], [0.9450980392156862], [0.984313725490196], [1.0], [0.9607843137254902], [1.0], [0.2980392156862745], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.9529411764705882], [0.9294117647058824], [0.8509803921568627], [0.8941176470588236], [0.9058823529411765], [0.8705882352941177], [0.8549019607843137], [0.8588235294117647], [1.0], [0.4549019607843137], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9215686274509803], [0.9058823529411765], [0.9137254901960784], [0.8862745098039215], [0.8823529411764706], [0.8980392156862745], [0.8705882352941177], [1.0], [0.5686274509803921], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.09019607843137255], [1.0], [0.9019607843137255], [0.8980392156862745], [0.9137254901960784], [0.8980392156862745], [0.8823529411764706], [0.8901960784313725], [0.8666666666666667], [0.9450980392156862], [0.6549019607843137], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.2627450980392157], [1.0], [0.8823529411764706], [0.9176470588235294], [0.9058823529411765], [0.8862745098039215], [0.8901960784313725], [0.8941176470588236], [0.8784313725490196], [0.9176470588235294], [0.7333333333333333], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.4470588235294118], [0.9764705882352941], [0.8509803921568627], [0.9215686274509803], [0.9333333333333333], [0.9607843137254902], [0.8901960784313725], [0.8901960784313725], [0.8823529411764706], [0.9450980392156862], [0.6901960784313725], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.6549019607843137], [0.9686274509803922], [0.8901960784313725], [0.9058823529411765], [0.9803921568627451], [0.7843137254901961], [0.9725490196078431], [0.9058823529411765], [0.8784313725490196], [0.984313725490196], [0.5764705882352941], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8156862745098039], [0.9490196078431372], [0.8823529411764706], [0.9529411764705882], [0.8823529411764706], [0.0], [1.0], [0.9137254901960784], [0.8862745098039215], [1.0], [0.5058823529411764], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8745098039215686], [0.9333333333333333], [0.8745098039215686], [1.0], [0.6313725490196078], [0.0], [1.0], [0.9254901960784314], [0.8745098039215686], [1.0], [0.5294117647058824], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.9607843137254902], [0.9215686274509803], [0.8705882352941177], [1.0], [0.2823529411764706], [0.0], [0.9725490196078431], [0.996078431372549], [0.8509803921568627], [1.0], [0.5686274509803921], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9137254901960784], [0.8862745098039215], [1.0], [0.027450980392156862], [0.0], [0.7490196078431373], [0.9725490196078431], [0.8627450980392157], [1.0], [0.49411764705882355], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9137254901960784], [0.9058823529411765], [0.984313725490196], [0.0], [0.0], [0.6235294117647059], [0.984313725490196], [0.8666666666666667], [1.0], [0.43529411764705883], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9098039215686274], [0.9254901960784314], [0.8470588235294118], [0.0], [0.0], [0.5137254901960784], [0.9921568627450981], [0.8627450980392157], [1.0], [0.43529411764705883], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.8941176470588236], [0.9529411764705882], [0.6745098039215687], [0.0], [0.0], [0.2235294117647059], [0.9764705882352941], [0.8705882352941177], [1.0], [0.43529411764705883], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9019607843137255], [0.9568627450980393], [0.5450980392156862], [0.0], [0.0], [0.0392156862745098], [1.0], [0.8901960784313725], [1.0], [0.39215686274509803], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8901960784313725], [0.9294117647058824], [0.9490196078431372], [0.44313725490196076], [0.0], [0.0], [0.023529411764705882], [1.0], [0.9019607843137255], [1.0], [0.34901960784313724], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8], [0.9372549019607843], [0.9607843137254902], [0.592156862745098], [0.0], [0.0], [0.0], [1.0], [0.8901960784313725], [1.0], [0.38823529411764707], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.592156862745098], [0.9607843137254902], [0.9333333333333333], [0.7764705882352941], [0.0], [0.0], [0.0], [1.0], [0.9176470588235294], [1.0], [0.3607843137254902], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.34901960784313724], [0.9725490196078431], [0.9137254901960784], [0.9725490196078431], [0.0], [0.0], [0.0], [0.9882352941176471], [0.9294117647058824], [1.0], [0.35294117647058826], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.12156862745098039], [0.9411764705882353], [0.8980392156862745], [0.8862745098039215], [0.0], [0.0], [0.0], [0.9372549019607843], [0.9333333333333333], [1.0], [0.3607843137254902], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8862745098039215], [0.9137254901960784], [0.9294117647058824], [0.13333333333333333], [0.0], [0.0], [0.9176470588235294], [0.9333333333333333], [1.0], [0.37254901960784315], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.9137254901960784], [0.9254901960784314], [0.9568627450980393], [0.26666666666666666], [0.0], [0.0], [0.8196078431372549], [0.9450980392156862], [0.9294117647058824], [0.3843137254901961], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.596078431372549], [0.9490196078431372], [0.9607843137254902], [0.5019607843137255], [0.0], [0.0], [0.7764705882352941], [0.9450980392156862], [0.9333333333333333], [0.3176470588235294], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00784313725490196], [0.0], [0.28627450980392155], [0.9647058823529412], [0.9450980392156862], [0.8274509803921568], [0.0], [0.0], [0.792156862745098], [0.9411764705882353], [0.9294117647058824], [0.2901960784313726], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.01568627450980392], [0.0], [0.0], [0.8980392156862745], [0.9254901960784314], [0.8196078431372549], [0.0], [0.0], [0.6196078431372549], [0.9686274509803922], [0.9333333333333333], [0.38823529411764707], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.7803921568627451], [1.0], [0.9686274509803922], [0.22745098039215686], [0.0], [0.6313725490196078], [1.0], [0.9882352941176471], [0.4666666666666667], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.3843137254901961], [0.6235294117647059], [0.2784313725490196], [0.0], [0.0], [0.26666666666666666], [0.6901960784313725], [0.6431372549019608], [0.22745098039215686], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]]]}'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.011764705882352941], [0.00392156862745098], [0.0], [0.0], [0.027450980392156862], [0.0], [0.1450980392156863], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.00784313725490196], [0.0], [0.10588235294117647], [0.32941176470588235], [0.043137254901960784], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.4666666666666667], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.34509803921568627], [0.5607843137254902], [0.43137254901960786], [0.0], [0.0], [0.0], [0.0], [0.08627450980392157], [0.36470588235294116], [0.41568627450980394], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.01568627450980392], [0.0], [0.20784313725490197], [0.5058823529411764], [0.47058823529411764], [0.5764705882352941], [0.6862745098039216], [0.615686274509804], [0.6509803921568628], [0.5294117647058824], [0.6039215686274509], [0.6588235294117647], [0.5490196078431373], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00784313725490196], [0.0], [0.043137254901960784], [0.5372549019607843], [0.5098039215686274], [0.5019607843137255], [0.6274509803921569], [0.6901960784313725], [0.6235294117647059], [0.6549019607843137], [0.6980392156862745], [0.5843137254901961], [0.592156862745098], [0.5647058823529412], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.00784313725490196], [0.00392156862745098], [0.0], [0.011764705882352941], [0.0], [0.0], [0.45098039215686275], [0.4470588235294118], [0.41568627450980394], [0.5372549019607843], [0.6588235294117647], [0.6], [0.611764705882353], [0.6470588235294118], [0.6549019607843137], [0.5607843137254902], [0.615686274509804], [0.6196078431372549], [0.043137254901960784], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.0], [0.0], [0.0], [0.011764705882352941], [0.0], [0.0], [0.34901960784313724], [0.5450980392156862], [0.35294117647058826], [0.3686274509803922], [0.6], [0.5843137254901961], [0.5137254901960784], [0.592156862745098], [0.6627450980392157], [0.6745098039215687], [0.5607843137254902], [0.6235294117647059], [0.6627450980392157], [0.18823529411764706], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00784313725490196], [0.01568627450980392], [0.00392156862745098], [0.0], [0.0], [0.0], [0.3843137254901961], [0.5333333333333333], [0.43137254901960786], [0.42745098039215684], [0.43137254901960786], [0.6352941176470588], [0.5294117647058824], [0.5647058823529412], [0.5843137254901961], [0.6235294117647059], [0.6549019607843137], [0.5647058823529412], [0.6196078431372549], [0.6627450980392157], [0.4666666666666667], [0.0]], [[0.0], [0.0], [0.00784313725490196], [0.00784313725490196], [0.00392156862745098], [0.00784313725490196], [0.0], [0.0], [0.0], [0.0], [0.10196078431372549], [0.4235294117647059], [0.4588235294117647], [0.38823529411764707], [0.43529411764705883], [0.4588235294117647], [0.5333333333333333], [0.611764705882353], [0.5254901960784314], [0.6039215686274509], [0.6039215686274509], [0.611764705882353], [0.6274509803921569], [0.5529411764705883], [0.5764705882352941], [0.611764705882353], [0.6980392156862745], [0.0]], [[0.011764705882352941], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.08235294117647059], [0.20784313725490197], [0.3607843137254902], [0.4588235294117647], [0.43529411764705883], [0.403921568627451], [0.45098039215686275], [0.5058823529411764], [0.5254901960784314], [0.5607843137254902], [0.6039215686274509], [0.6470588235294118], [0.6666666666666666], [0.6039215686274509], [0.592156862745098], [0.6039215686274509], [0.5607843137254902], [0.5411764705882353], [0.5882352941176471], [0.6470588235294118], [0.16862745098039217]], [[0.0], [0.0], [0.09019607843137255], [0.21176470588235294], [0.2549019607843137], [0.2980392156862745], [0.3333333333333333], [0.4627450980392157], [0.5019607843137255], [0.4823529411764706], [0.43529411764705883], [0.44313725490196076], [0.4627450980392157], [0.4980392156862745], [0.49019607843137253], [0.5450980392156862], [0.5215686274509804], [0.5333333333333333], [0.6274509803921569], [0.5490196078431373], [0.6078431372549019], [0.6313725490196078], [0.5647058823529412], [0.6078431372549019], [0.6745098039215687], [0.6313725490196078], [0.7411764705882353], [0.24313725490196078]], [[0.0], [0.26666666666666666], [0.3686274509803922], [0.35294117647058826], [0.43529411764705883], [0.4470588235294118], [0.43529411764705883], [0.4470588235294118], [0.45098039215686275], [0.4980392156862745], [0.5294117647058824], [0.5333333333333333], [0.5607843137254902], [0.49411764705882355], [0.4980392156862745], [0.592156862745098], [0.6039215686274509], [0.5607843137254902], [0.5803921568627451], [0.49019607843137253], [0.6352941176470588], [0.6352941176470588], [0.5647058823529412], [0.5411764705882353], [0.6], [0.6352941176470588], [0.7686274509803922], [0.22745098039215686]], [[0.27450980392156865], [0.6627450980392157], [0.5058823529411764], [0.40784313725490196], [0.3843137254901961], [0.39215686274509803], [0.3686274509803922], [0.3803921568627451], [0.3843137254901961], [0.4], [0.4235294117647059], [0.41568627450980394], [0.4666666666666667], [0.47058823529411764], [0.5058823529411764], [0.5843137254901961], [0.611764705882353], [0.6549019607843137], [0.7450980392156863], [0.7450980392156863], [0.7686274509803922], [0.7764705882352941], [0.7764705882352941], [0.7333333333333333], [0.7725490196078432], [0.7411764705882353], [0.7215686274509804], [0.1411764705882353]], [[0.06274509803921569], [0.49411764705882355], [0.6705882352941176], [0.7372549019607844], [0.7372549019607844], [0.7215686274509804], [0.6705882352941176], [0.6], [0.5294117647058824], [0.47058823529411764], [0.49411764705882355], [0.4980392156862745], [0.5725490196078431], [0.7254901960784313], [0.7647058823529411], [0.8196078431372549], [0.8156862745098039], [1.0], [0.8196078431372549], [0.6941176470588235], [0.9607843137254902], [0.9882352941176471], [0.984313725490196], [0.984313725490196], [0.9686274509803922], [0.8627450980392157], [0.807843137254902], [0.19215686274509805]], [[0.0], [0.0], [0.0], [0.047058823529411764], [0.2627450980392157], [0.41568627450980394], [0.6431372549019608], [0.7254901960784313], [0.7803921568627451], [0.8235294117647058], [0.8274509803921568], [0.8235294117647058], [0.8156862745098039], [0.7450980392156863], [0.5882352941176471], [0.3215686274509804], [0.03137254901960784], [0.0], [0.0], [0.0], [0.6980392156862745], [0.8156862745098039], [0.7372549019607844], [0.6862745098039216], [0.6352941176470588], [0.6196078431372549], [0.592156862745098], [0.043137254901960784]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]], [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.050980392156862744], [0.2627450980392157], [0.0], [0.0], [0.0], [0.0], [0.19607843137254902], [0.14901960784313725], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.03137254901960784], [0.47058823529411764], [0.8196078431372549], [0.8862745098039215], [0.9686274509803922], [0.9294117647058824], [1.0], [1.0], [1.0], [0.9686274509803922], [0.9333333333333333], [0.9215686274509803], [0.6745098039215687], [0.2823529411764706], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.5372549019607843], [0.9372549019607843], [0.9882352941176471], [0.9529411764705882], [0.9176470588235294], [0.8980392156862745], [0.9333333333333333], [0.9568627450980393], [0.9647058823529412], [0.9411764705882353], [0.9019607843137255], [0.9098039215686274], [0.9372549019607843], [0.9725490196078431], [0.984313725490196], [0.7607843137254902], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.4], [1.0], [0.9058823529411765], [0.8941176470588236], [0.8901960784313725], [0.8941176470588236], [0.9137254901960784], [0.9019607843137255], [0.9019607843137255], [0.8980392156862745], [0.8941176470588236], [0.9098039215686274], [0.9098039215686274], [0.9058823529411765], [0.8901960784313725], [0.8784313725490196], [0.9882352941176471], [0.7019607843137254], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.9137254901960784], [0.9450980392156862], [0.8980392156862745], [0.9058823529411765], [1.0], [1.0], [0.9333333333333333], [0.9058823529411765], [0.8901960784313725], [0.9333333333333333], [0.9647058823529412], [0.8941176470588236], [0.9019607843137255], [0.8901960784313725], [0.9176470588235294], [0.9215686274509803], [0.8980392156862745], [0.9450980392156862], [0.0784313725490196], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.9725490196078431], [0.9450980392156862], [0.9058823529411765], [1.0], [0.5843137254901961], [0.1843137254901961], [0.9882352941176471], [0.8941176470588236], [1.0], [0.9490196078431372], [0.8470588235294118], [0.9333333333333333], [0.9098039215686274], [1.0], [0.8941176470588236], [0.8627450980392157], [0.9176470588235294], [0.9803921568627451], [0.21176470588235294], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9411764705882353], [0.9098039215686274], [1.0], [0.058823529411764705], [0.0], [1.0], [0.9294117647058824], [0.7490196078431373], [0.0], [0.0], [0.8392156862745098], [1.0], [0.050980392156862744], [0.4823529411764706], [1.0], [0.9176470588235294], [0.9882352941176471], [0.4470588235294118], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.023529411764705882], [1.0], [0.9333333333333333], [0.9372549019607843], [1.0], [0.6941176470588235], [0.0], [1.0], [1.0], [0.0], [0.5098039215686274], [0.4549019607843137], [0.1843137254901961], [0.2549019607843137], [0.16862745098039217], [0.1450980392156863], [1.0], [0.9254901960784314], [0.9764705882352941], [0.6352941176470588], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.12549019607843137], [1.0], [0.9254901960784314], [0.9607843137254902], [1.0], [0.8], [0.0], [1.0], [0.32941176470588235], [0.0], [0.1450980392156863], [0.10980392156862745], [0.12156862745098039], [0.0], [0.09803921568627451], [0.050980392156862744], [1.0], [0.9254901960784314], [0.9764705882352941], [0.7803921568627451], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.20784313725490197], [1.0], [0.9254901960784314], [0.9803921568627451], [0.9803921568627451], [0.9058823529411765], [0.00784313725490196], [1.0], [0.08235294117647059], [0.0], [0.8666666666666667], [1.0], [0.9254901960784314], [0.21176470588235294], [0.9607843137254902], [0.7764705882352941], [0.9529411764705882], [0.9333333333333333], [0.9607843137254902], [0.8745098039215686], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.3137254901960784], [1.0], [0.9294117647058824], [0.9803921568627451], [0.9411764705882353], [1.0], [0.0], [0.0], [0.15294117647058825], [0.615686274509804], [0.0], [0.0], [0.8431372549019608], [0.3686274509803922], [0.0784313725490196], [0.49411764705882355], [1.0], [0.9294117647058824], [0.9372549019607843], [0.9803921568627451], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.396078431372549], [1.0], [0.9215686274509803], [0.9921568627450981], [0.9568627450980393], [0.9529411764705882], [0.5215686274509804], [0.5411764705882353], [0.8156862745098039], [1.0], [0.788235294117647], [0.8392156862745098], [1.0], [0.9019607843137255], [0.027450980392156862], [0.6823529411764706], [1.0], [0.9411764705882353], [0.9333333333333333], [1.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.49411764705882355], [1.0], [0.9137254901960784], [1.0], [0.9725490196078431], [0.9137254901960784], [1.0], [1.0], [0.9411764705882353], [0.9098039215686274], [0.9529411764705882], [0.9529411764705882], [0.9058823529411765], [0.984313725490196], [1.0], [1.0], [0.996078431372549], [0.9529411764705882], [0.9333333333333333], [1.0], [0.011764705882352941], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.5764705882352941], [1.0], [0.9137254901960784], [0.9764705882352941], [0.7098039215686275], [0.9529411764705882], [0.8901960784313725], [0.8784313725490196], [0.9019607843137255], [0.9176470588235294], [0.9019607843137255], [0.9019607843137255], [0.9215686274509803], [0.8941176470588236], [0.9215686274509803], [0.8705882352941177], [0.8117647058823529], [1.0], [0.9254901960784314], [1.0], [0.13725490196078433], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.6392156862745098], [1.0], [0.9607843137254902], [0.8666666666666667], [0.33725490196078434], [1.0], [0.9137254901960784], [0.9137254901960784], [0.9215686274509803], [0.9254901960784314], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9098039215686274], [0.9490196078431372], [0.9058823529411765], [0.49019607843137253], [1.0], [0.9254901960784314], [1.0], [0.21568627450980393], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7098039215686275], [0.996078431372549], [1.0], [0.7843137254901961], [0.27058823529411763], [1.0], [0.8941176470588236], [0.9098039215686274], [0.9176470588235294], [0.9215686274509803], [0.9176470588235294], [0.9176470588235294], [0.9137254901960784], [0.9215686274509803], [0.9450980392156862], [0.9294117647058824], [0.27450980392156865], [1.0], [0.9215686274509803], [0.9647058823529412], [0.2235294117647059], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7725490196078432], [0.9686274509803922], [1.0], [0.7372549019607844], [0.43137254901960786], [1.0], [0.8784313725490196], [0.9137254901960784], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9411764705882353], [0.9921568627450981], [0.27058823529411763], [1.0], [0.9254901960784314], [0.9725490196078431], [0.30196078431372547], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7843137254901961], [0.9647058823529412], [1.0], [0.5843137254901961], [0.5686274509803921], [1.0], [0.8745098039215686], [0.9215686274509803], [0.9176470588235294], [0.9215686274509803], [0.9215686274509803], [0.9215686274509803], [0.9176470588235294], [0.9294117647058824], [0.9137254901960784], [1.0], [0.1843137254901961], [1.0], [0.9372549019607843], [0.9764705882352941], [0.3843137254901961], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.8], [0.9529411764705882], [1.0], [0.43529411764705883], [0.6784313725490196], [1.0], [0.8901960784313725], [0.9215686274509803], [0.9215686274509803], [0.9254901960784314], [0.9215686274509803], [0.9215686274509803], [0.9215686274509803], [0.9372549019607843], [0.8980392156862745], [1.0], [0.07450980392156863], [0.8901960784313725], [0.9647058823529412], [0.9764705882352941], [0.43137254901960786], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7686274509803922], [0.9411764705882353], [1.0], [0.42745098039215684], [0.8352941176470589], [0.9803921568627451], [0.8980392156862745], [0.9215686274509803], [0.9215686274509803], [0.9254901960784314], [0.9215686274509803], [0.9294117647058824], [0.9254901960784314], [0.9294117647058824], [0.8862745098039215], [1.0], [0.21568627450980393], [0.796078431372549], [0.984313725490196], [0.9607843137254902], [0.47058823529411764], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7529411764705882], [0.9529411764705882], [1.0], [0.4470588235294118], [0.9098039215686274], [0.9411764705882353], [0.9098039215686274], [0.9215686274509803], [0.9215686274509803], [0.9254901960784314], [0.9176470588235294], [0.9294117647058824], [0.9254901960784314], [0.9215686274509803], [0.8980392156862745], [1.0], [0.5254901960784314], [0.6705882352941176], [0.9882352941176471], [0.9568627450980393], [0.5372549019607843], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7411764705882353], [0.984313725490196], [1.0], [0.6039215686274509], [0.9333333333333333], [0.9137254901960784], [0.9254901960784314], [0.9176470588235294], [0.9215686274509803], [0.9254901960784314], [0.9215686274509803], [0.9333333333333333], [0.9254901960784314], [0.9215686274509803], [0.9098039215686274], [1.0], [0.6509803921568628], [0.49019607843137253], [1.0], [0.9529411764705882], [0.5568627450980392], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7176470588235294], [0.9882352941176471], [1.0], [0.6705882352941176], [0.9686274509803922], [0.9098039215686274], [0.9176470588235294], [0.9176470588235294], [0.9137254901960784], [0.9137254901960784], [0.9098039215686274], [0.9176470588235294], [0.9137254901960784], [0.9176470588235294], [0.9137254901960784], [0.9411764705882353], [0.8745098039215686], [0.5019607843137255], [1.0], [0.9490196078431372], [0.592156862745098], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.6980392156862745], [0.9529411764705882], [1.0], [0.2235294117647059], [0.9333333333333333], [0.9450980392156862], [0.9333333333333333], [0.9333333333333333], [0.9333333333333333], [0.9294117647058824], [0.9254901960784314], [0.9294117647058824], [0.9294117647058824], [0.9411764705882353], [0.9294117647058824], [0.996078431372549], [0.6901960784313725], [0.20392156862745098], [1.0], [0.9372549019607843], [0.615686274509804], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7372549019607844], [0.9411764705882353], [0.9803921568627451], [0.24313725490196078], [0.8549019607843137], [1.0], [0.8627450980392157], [0.8705882352941177], [0.8705882352941177], [0.8705882352941177], [0.8745098039215686], [0.8745098039215686], [0.8784313725490196], [0.8705882352941177], [0.8549019607843137], [1.0], [0.6039215686274509], [0.12549019607843137], [1.0], [0.9254901960784314], [0.7372549019607844], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.5098039215686274], [0.9607843137254902], [0.9490196078431372], [0.09411764705882353], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.13333333333333333], [0.9490196078431372], [0.9568627450980393], [0.5294117647058824], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.2980392156862745], [1.0], [0.9764705882352941], [0.08627450980392157], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.15294117647058825], [0.9764705882352941], [1.0], [0.4823529411764706], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.19215686274509805], [0.803921568627451], [0.7725490196078432], [0.043137254901960784], [0.0], [0.01568627450980392], [0.00392156862745098], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.011764705882352941], [0.0], [0.011764705882352941], [0.6823529411764706], [0.7411764705882353], [0.2627450980392157], [0.0], [0.0], [0.0]]], [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.2627450980392157], [0.6941176470588235], [0.5058823529411764], [0.6], [0.4588235294117647], [0.5058823529411764], [0.5725490196078431], [0.5529411764705883], [0.6862745098039216], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00784313725490196], [0.0], [0.7686274509803922], [1.0], [1.0], [1.0], [0.9450980392156862], [0.984313725490196], [1.0], [0.9607843137254902], [1.0], [0.2980392156862745], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.9529411764705882], [0.9294117647058824], [0.8509803921568627], [0.8941176470588236], [0.9058823529411765], [0.8705882352941177], [0.8549019607843137], [0.8588235294117647], [1.0], [0.4549019607843137], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9215686274509803], [0.9058823529411765], [0.9137254901960784], [0.8862745098039215], [0.8823529411764706], [0.8980392156862745], [0.8705882352941177], [1.0], [0.5686274509803921], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.09019607843137255], [1.0], [0.9019607843137255], [0.8980392156862745], [0.9137254901960784], [0.8980392156862745], [0.8823529411764706], [0.8901960784313725], [0.8666666666666667], [0.9450980392156862], [0.6549019607843137], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.2627450980392157], [1.0], [0.8823529411764706], [0.9176470588235294], [0.9058823529411765], [0.8862745098039215], [0.8901960784313725], [0.8941176470588236], [0.8784313725490196], [0.9176470588235294], [0.7333333333333333], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.4470588235294118], [0.9764705882352941], [0.8509803921568627], [0.9215686274509803], [0.9333333333333333], [0.9607843137254902], [0.8901960784313725], [0.8901960784313725], [0.8823529411764706], [0.9450980392156862], [0.6901960784313725], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.6549019607843137], [0.9686274509803922], [0.8901960784313725], [0.9058823529411765], [0.9803921568627451], [0.7843137254901961], [0.9725490196078431], [0.9058823529411765], [0.8784313725490196], [0.984313725490196], [0.5764705882352941], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8156862745098039], [0.9490196078431372], [0.8823529411764706], [0.9529411764705882], [0.8823529411764706], [0.0], [1.0], [0.9137254901960784], [0.8862745098039215], [1.0], [0.5058823529411764], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8745098039215686], [0.9333333333333333], [0.8745098039215686], [1.0], [0.6313725490196078], [0.0], [1.0], [0.9254901960784314], [0.8745098039215686], [1.0], [0.5294117647058824], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.9607843137254902], [0.9215686274509803], [0.8705882352941177], [1.0], [0.2823529411764706], [0.0], [0.9725490196078431], [0.996078431372549], [0.8509803921568627], [1.0], [0.5686274509803921], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9137254901960784], [0.8862745098039215], [1.0], [0.027450980392156862], [0.0], [0.7490196078431373], [0.9725490196078431], [0.8627450980392157], [1.0], [0.49411764705882355], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9137254901960784], [0.9058823529411765], [0.984313725490196], [0.0], [0.0], [0.6235294117647059], [0.984313725490196], [0.8666666666666667], [1.0], [0.43529411764705883], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9098039215686274], [0.9254901960784314], [0.8470588235294118], [0.0], [0.0], [0.5137254901960784], [0.9921568627450981], [0.8627450980392157], [1.0], [0.43529411764705883], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.8941176470588236], [0.9529411764705882], [0.6745098039215687], [0.0], [0.0], [0.2235294117647059], [0.9764705882352941], [0.8705882352941177], [1.0], [0.43529411764705883], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9019607843137255], [0.9568627450980393], [0.5450980392156862], [0.0], [0.0], [0.0392156862745098], [1.0], [0.8901960784313725], [1.0], [0.39215686274509803], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8901960784313725], [0.9294117647058824], [0.9490196078431372], [0.44313725490196076], [0.0], [0.0], [0.023529411764705882], [1.0], [0.9019607843137255], [1.0], [0.34901960784313724], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8], [0.9372549019607843], [0.9607843137254902], [0.592156862745098], [0.0], [0.0], [0.0], [1.0], [0.8901960784313725], [1.0], [0.38823529411764707], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.592156862745098], [0.9607843137254902], [0.9333333333333333], [0.7764705882352941], [0.0], [0.0], [0.0], [1.0], [0.9176470588235294], [1.0], [0.3607843137254902], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.34901960784313724], [0.9725490196078431], [0.9137254901960784], [0.9725490196078431], [0.0], [0.0], [0.0], [0.9882352941176471], [0.9294117647058824], [1.0], [0.35294117647058826], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.12156862745098039], [0.9411764705882353], [0.8980392156862745], [0.8862745098039215], [0.0], [0.0], [0.0], [0.9372549019607843], [0.9333333333333333], [1.0], [0.3607843137254902], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8862745098039215], [0.9137254901960784], [0.9294117647058824], [0.13333333333333333], [0.0], [0.0], [0.9176470588235294], [0.9333333333333333], [1.0], [0.37254901960784315], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.9137254901960784], [0.9254901960784314], [0.9568627450980393], [0.26666666666666666], [0.0], [0.0], [0.8196078431372549], [0.9450980392156862], [0.9294117647058824], [0.3843137254901961], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.596078431372549], [0.9490196078431372], [0.9607843137254902], [0.5019607843137255], [0.0], [0.0], [0.7764705882352941], [0.9450980392156862], [0.9333333333333333], [0.3176470588235294], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00784313725490196], [0.0], [0.28627450980392155], [0.9647058823529412], [0.9450980392156862], [0.8274509803921568], [0.0], [0.0], [0.792156862745098], [0.9411764705882353], [0.9294117647058824], [0.2901960784313726], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.01568627450980392], [0.0], [0.0], [0.8980392156862745], [0.9254901960784314], [0.8196078431372549], [0.0], [0.0], [0.6196078431372549], [0.9686274509803922], [0.9333333333333333], [0.38823529411764707], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.7803921568627451], [1.0], [0.9686274509803922], [0.22745098039215686], [0.0], [0.6313725490196078], [1.0], [0.9882352941176471], [0.4666666666666667], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.3843137254901961], [0.6235294117647059], [0.2784313725490196], [0.0], [0.0], [0.26666666666666666], [0.6901960784313725], [0.6431372549019608], [0.22745098039215686], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]]]\n"
     ]
    }
   ],
   "source": [
    "print(test_images[0:3].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images[0:3][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 1)\n",
      "[[[1], [2], [3]], [[4], [5], [6]]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array([[[1], [2], [3]], [[4], [5], [6]]]).shape)\n",
    "print(np.array([[[1], [2], [3]], [[4], [5], [6]]]).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export_path = 1\n",
      "INFO:tensorflow:Assets written to: 1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '1'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  5395780528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5393388992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5395058176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5395055888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Saved model:\n",
      "total 80\n",
      "drwxr-xr-x  2 dgmr2  staff     64 Jan 15 22:46 \u001b[34massets\u001b[m\u001b[m\n",
      "-rw-r--r--  1 dgmr2  staff     59 Jan 15 22:46 fingerprint.pb\n",
      "-rw-r--r--  1 dgmr2  staff  34669 Jan 15 22:46 saved_model.pb\n",
      "drwxr-xr-x  4 dgmr2  staff    128 Jan 15 22:46 \u001b[34mvariables\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# MODEL_DIR = tempfile.gettempdir()\n",
    "# version = 1\n",
    "# export_path = os.path.join(MODEL_DIR, str(version))\n",
    "export_path = '1'\n",
    "print(f'export_path = {export_path}')\n",
    "\n",
    "model.export(export_path, format='tf_saved_model', verbose=True)\n",
    "\n",
    "print('Saved model:')\n",
    "!ls -l {export_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['serve', 'serving_default']\n"
     ]
    }
   ],
   "source": [
    "loaded = tf.saved_model.load(export_path)\n",
    "print(list(loaded.signatures.keys()))  # [\"serving_default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_0': TensorSpec(shape=(None, 10), dtype=tf.float32, name='output_0')}\n"
     ]
    }
   ],
   "source": [
    "infer = loaded.signatures[\"serving_default\"]\n",
    "print(infer.structured_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtomicFunction(name=b'__inference_signature_wrapper___call___43324',\n",
       "bound_context=Eager TensorFlow Context with 1 devices\n",
       "   Device 0: /job:localhost/replica:0/task:0/device:CPU:0,\n",
       "function_type=Input Parameters:\n",
       "  keras_tensor (KEYWORD_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='keras_tensor')\n",
       "Output Type:\n",
       "  Dict[['output_0', TensorSpec(shape=(None, 10), dtype=tf.float32, name='output_0')]]\n",
       "Captures:\n",
       "  None,\n",
       "children=[AtomicFunction(name=b'__inference___call___43315',\n",
       "bound_context=Eager TensorFlow Context with 1 devices\n",
       "   Device 0: /job:localhost/replica:0/task:0/device:CPU:0,\n",
       "function_type=Input Parameters:\n",
       "  arg_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='keras_tensor')\n",
       "Output Type:\n",
       "  TensorSpec(shape=(None, 10), dtype=tf.float32, name='unknown')\n",
       "Captures:\n",
       "  None,\n",
       "children=[],\n",
       "call_options=CallOptions(collective_manager_ids_used=[], control_captures=<tensorflow.python.util.object_identity.ObjectIdentitySet object at 0x133b1d0d0>, is_stateful=True),\n",
       "cached_graph=FuncGraph(name=__call__, id=5497124032))],\n",
       "call_options=CallOptions(collective_manager_ids_used=[], control_captures=<tensorflow.python.util.object_identity.ObjectIdentitySet object at 0x146eccb80>, is_stateful=True),\n",
       "cached_graph=FuncGraph(name=signature_wrapper___call__, id=5495305792))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer.inference_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on _WrapperFunction in module tensorflow.python.saved_model.load object:\n",
      "\n",
      "class _WrapperFunction(tensorflow.python.eager.polymorphic_function.concrete_function.ConcreteFunction)\n",
      " |  _WrapperFunction(concrete_function)\n",
      " |  \n",
      " |  A class wraps a concrete function to handle different distributed contexts.\n",
      " |  \n",
      " |  The reason for wrapping a concrete function is because the _captured_inputs\n",
      " |  fields used for in-replica context and cross-replica context are different.\n",
      " |  When `load()` is called from within a tf.distribute.strategy scope, the\n",
      " |  captured inputs are distributed variables. When using these distributed\n",
      " |  variables during calling the function, we need different approaches when it is\n",
      " |  in-replica and when it is not in-replica. When it is in replica, naturally we\n",
      " |  should use the corresponding component of the distributed variable; when it is\n",
      " |  not in-replica, calling the function should mean that it is constructing a\n",
      " |  graph that is not actually going to be used. A typical use case is when\n",
      " |  constructing a functional model. In this case, return a placeholder with a\n",
      " |  control dependency to ensure that is never accessed.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      _WrapperFunction\n",
      " |      tensorflow.python.eager.polymorphic_function.concrete_function.ConcreteFunction\n",
      " |      tensorflow.python.types.core.ConcreteFunction\n",
      " |      tensorflow.python.types.core.Callable\n",
      " |      tensorflow.python.trackable.base.Trackable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, concrete_function)\n",
      " |      Initialize a `ConcreteFunction`.\n",
      " |      \n",
      " |      Args:\n",
      " |       atomic_fn: Inference atomic function to form basis of forward pass.\n",
      " |       shared_func_graph: If False, the ConcreteFunction takes ownership of\n",
      " |         `func_graph` and will break reference cycles when it is deleted. This\n",
      " |         makes the FuncGraph inoperable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If number of input_placeholders is not equal to the number\n",
      " |          of function inputs.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.eager.polymorphic_function.concrete_function.ConcreteFunction:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Executes the wrapped function.\n",
      " |      \n",
      " |      ConcreteFunctions have two signatures:\n",
      " |      \n",
      " |      * The signature of the original function wrapped by this ConcreteFunction.\n",
      " |      * A flat signature, where each argument accepts a single Tensor.\n",
      " |      \n",
      " |      The original function signature is generally preferred, but the flat input\n",
      " |      signature is supported for backward compatibility.\n",
      " |      \n",
      " |      ### Original Function Signature\n",
      " |      \n",
      " |      When calling a ConcreteFunction with the signature of the original function,\n",
      " |      each argument must match the type or value that was used when the\n",
      " |      ConcreteFunction's graph was traced.  In particular:\n",
      " |      \n",
      " |      * Tensor arguments (including CompositeTensors, such as RaggedTensor) must\n",
      " |        have matching `TypeSpec`s.\n",
      " |      * Non-Tensor arguments (such as booleans or ints) must have equal values.\n",
      " |      * Nested arguments (such as lists, tuples, or dictionaries) must have the\n",
      " |        same nesting structure; and each nested value must have a matching type\n",
      " |        or value.\n",
      " |      \n",
      " |      The default value for any arguments that were traced with non-Tensor values\n",
      " |      is the value that was used in the trace.  Arguments that were traced with\n",
      " |      tensor arguments do not have a default value (even if the original function\n",
      " |      had a default value for that argument).\n",
      " |      \n",
      " |      ### Flat Signature\n",
      " |      \n",
      " |      When calling a ConcreteFunction with the flat signature, the arguments\n",
      " |      correspond to the flattened component tensors of the arguments that were\n",
      " |      used to construct the ConcreteFunction.  Parameter names are assigned based\n",
      " |      on `TensorSpec.name` (when specified) or the original argument names (with\n",
      " |      suffixes automatically added for nested arguments or composite tensors with\n",
      " |      multiple components).\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: Positional arguments to the concrete function.\n",
      " |        **kwargs: Keyword arguments to the concrete function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The result of applying the TF function on the given Tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AssertionError: If this `ConcreteFunction` was not created through\n",
      " |          `get_concrete_function`.\n",
      " |        TypeError: If the arguments do not match the function's signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_gradient_functions_to_graph(self, g=None)\n",
      " |      Add forward/backward functions to graph `g` or the current context.\n",
      " |  \n",
      " |  add_to_graph(self, g=None, overwrite=False)\n",
      " |      Registers the function, adds it to the graph g or default graph.\n",
      " |      \n",
      " |      Args:\n",
      " |        g: If specified, registers the function with this graph. Defaults to the\n",
      " |          current context (either the default graph or the eager context).\n",
      " |        overwrite: A bool. If True, its forward function will overwrite\n",
      " |          any existing function of the same signature name in the graph `g`.\n",
      " |  \n",
      " |  pretty_printed_signature(self, verbose=True)\n",
      " |      Returns a string summarizing the signature of this concrete function.\n",
      " |  \n",
      " |  replace_capture_with_deferred_capture(self, tensor, closure, spec, placeholder=None, default_value=None)\n",
      " |      Replaces existing capture `tensor` with a deferred capture `closure`.\n",
      " |      \n",
      " |      This API replaces the capture `tensor` from the concrete function's captured\n",
      " |      inputs list, and places the deferred capture `closure` in\n",
      " |      its spot so the order of captured inputs is preserved. This is important\n",
      " |      because the old `tensor` and the new `closure` will have the same internal\n",
      " |      placeholder, which can be passed through the `placeholder` argument, or\n",
      " |      skipped, in which case we find the placeholder from internal inputs by\n",
      " |      indexing `tensor` in the external captured inputs list. Thus, it is\n",
      " |      important that the new deferred capture has output spec (specified by the\n",
      " |      `spec` argument) compatible with the internal placeholder (`placeholder`)\n",
      " |      and the original capture (`tensor`).\n",
      " |      \n",
      " |      For example,\n",
      " |      \n",
      " |      ```python\n",
      " |      bool_captured_tensor = tf.constant(True)\n",
      " |      float_captured_tensor = tf.constant([3.], dtype=tf.float32)\n",
      " |      value = tf.constant([2.], dtype=tf.float32)\n",
      " |      \n",
      " |      @tf.function\n",
      " |      def fn():\n",
      " |        deferred_tensor = ops.get_default_graph().capture_call_time_value(\n",
      " |            lambda: value,\n",
      " |            tf.TensorSpec(shape=(1,), dtype=tf.float32))\n",
      " |        if bool_captured_tensor:\n",
      " |          return deferred_tensor\n",
      " |        else:\n",
      " |          return deferred_tensor + float_captured_tensor\n",
      " |      \n",
      " |      concrete_fn = fn.get_concrete_function()\n",
      " |      print(concrete_fn())  # tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      " |      \n",
      " |      new_bool_captured_tensor = constant_op.constant(False)\n",
      " |      def bool_closure():\n",
      " |        return new_bool_captured_tensor\n",
      " |      \n",
      " |      concrete_fn.replace_capture_with_deferred_capture(\n",
      " |          bool_captured_tensor,\n",
      " |          bool_closure,\n",
      " |          spec=tensor_lib.TensorSpec(shape=(), dtype=dtypes.bool))\n",
      " |      \n",
      " |      print(concrete_fn())  # tf.Tensor([5.], shape=(1,), dtype=float32)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        tensor: Tensor already captured. This `tensor` should be listed in\n",
      " |          concrete_function.captured_inputs except when it's empty such as when\n",
      " |          the concrete function is restored from SavedModel.\n",
      " |        closure: function which takes no arguments, to be evaluated at function\n",
      " |          call time, returning a nest of tensors compatible with `spec`.\n",
      " |        spec: nest of TypeSpec for the value to capture.\n",
      " |        placeholder: optional. The internal placeholder corresponding to the\n",
      " |          captured `tensor` and the new `closure`.\n",
      " |        default_value: optional value to use in environments that cannot safely\n",
      " |          evaluate closure.\n",
      " |  \n",
      " |  set_external_captures(self, captures)\n",
      " |      Updates the function capture values.\n",
      " |      \n",
      " |      The new values must have tensor types and shapes consistent with the\n",
      " |      original captures of the concrete function, but it is allowed to change a\n",
      " |      value captured with a deferred one and vice-versa.\n",
      " |      \n",
      " |      Args:\n",
      " |        captures: A list of tensors or closures. Tensors are value captures, and\n",
      " |          closures are call-time (deferred captures).\n",
      " |  \n",
      " |  set_variables(self, variables)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.eager.polymorphic_function.concrete_function.ConcreteFunction:\n",
      " |  \n",
      " |  from_func_graph(graph, function_type, attrs, shared_func_graph=True) from abc.ABCMeta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow.python.eager.polymorphic_function.concrete_function.ConcreteFunction:\n",
      " |  \n",
      " |  captured_inputs\n",
      " |      Returns external Tensors captured by this function.\n",
      " |      \n",
      " |      self.__call__(*args) passes `args + self.captured_inputs` to the function.\n",
      " |  \n",
      " |  function_def\n",
      " |      Returns a `FunctionDef` object representing this function.\n",
      " |  \n",
      " |  function_type\n",
      " |      Return the FunctionType associated with this ConcreteFunction.\n",
      " |  \n",
      " |  graph\n",
      " |      Returns the graph from which this function was constructed.\n",
      " |  \n",
      " |  inference_fn\n",
      " |      Return the inference function associated with this ConcreteFunction.\n",
      " |  \n",
      " |  inputs\n",
      " |      Returns tensors in `self.graph` corresponding to arguments.\n",
      " |  \n",
      " |  name\n",
      " |      `ConcreteFunction` name.\n",
      " |  \n",
      " |  output_dtypes\n",
      " |  \n",
      " |  output_shapes\n",
      " |      The function's output shapes.\n",
      " |  \n",
      " |  outputs\n",
      " |      Returns tensors in `self.graph` corresponding to returned tensors.\n",
      " |  \n",
      " |  structured_input_signature\n",
      " |      Returns structured signature for this concrete function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tuple `(args, kwargs)`, where:\n",
      " |      \n",
      " |          * `args` is a tuple that specifies the expected type or value each for\n",
      " |            positional argument.\n",
      " |          * `kwargs` is a dictionary that specifies the expected type or value\n",
      " |            for each keyword-only argument.\n",
      " |      \n",
      " |        The type or value for each argument is specified using one of the\n",
      " |        following:\n",
      " |      \n",
      " |          * A `tf.TypeSpec`, indicating that a Tensor or other TensorFlow-native\n",
      " |            value is expected.\n",
      " |          * A Python value, such as an integer, indicating that an equal value\n",
      " |            is expected.\n",
      " |          * A nested structure of `tf.TypeSpec`s and Python values, indicating\n",
      " |            that a corresponding nested structure is expected.\n",
      " |  \n",
      " |  structured_outputs\n",
      " |      Returns outputs in `self.graph` as returned by the original function.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables for this function.\n",
      " |  \n",
      " |  variables\n",
      " |      Sequence of variables for this function.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.types.core.Callable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_UserObject' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m(test_images, test_labels)\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_UserObject' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "loaded_model.(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File format not supported: filepath=1. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(1, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/notebook/tensorflow/.venv/lib/python3.9/site-packages/keras/src/saving/saving_api.py:206\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy H5 format files (`.h5` extension). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that the legacy SavedModel format is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported by `load_model()` in Keras 3. In \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder to reload a TensorFlow SavedModel as an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference-only layer in Keras 3, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`keras.layers.TFSMLayer(\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, call_endpoint=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserving_default\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(note that your `call_endpoint` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: File format not supported: filepath=1. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(1, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name)."
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model(export_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
